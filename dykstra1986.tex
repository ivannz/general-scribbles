\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[mathcal]{euscript}

\usepackage{url}

\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\T}{\mathrm{T}}

\title{Some notes on Boyle, Dykstra (1986)}
\author{Nazarov Ivan}

\date{\today}

\begin{document}
\maketitle

Let $(\Hcal, \langle\cdot,\cdot\rangle)$ be a Hilbert space. We say that $(x_n)_{n\geq1} 
\in \Hcal$ converges {\bf strongly} to $x\in\Hcal$ if $\|x_n - x\| \to 0$. A sequence 
converges to $x$ weakly, if $\langle x_n, z \rangle \to \langle x, z\rangle$ for all
$z\in \Hcal$. Due to continuity of $\langle\cdot,\cdot\rangle$ w.r.t the product norm
topology and $\real$, strong convergence implies weak convergence.

A property of sequences, crucial for optimization problems in finite-dimensional spaces,
is strong sequential compactness: that every bounded sequence has a stringly convergent
subsequence (Bolzano-Weierstrass theorem). But in finite dimensional spaces it fails to
hold.

Let $C \subseteq \Hcal$ be a non-empty closed convex subset of $\Hcal$. Then for any
$g\in \Hcal$ there exists $g^*\in C$ such that $\|g - g^*\| = \inf_{h\in C} \|g - h\|$,
and for any $h\in C$ we have $\langle g - g^*, g^* - h\rangle \geq 0$. The latter follows
from convexity of $C$ and differentiability: in the following let $f = g^* + t (h -g ^*)$
and observe that $f \in C$ and it we have $\|f - g\| \geq \|g^* - g\|$ for all $t\in
[0, 1]$ (and in the limit $t\downarrow 0$)
\begin{equation*}
  \|f - g\|^2
    = \|f - g^*\|^2 + 2 \langle f - g^*, g^* - g\rangle + \|g^* - g\|^2
    % \geq \|f - g^*\|^2 + \|g^* - g\|^2
    \,.
\end{equation*}

Let $K$ non-empty closed convex sets $C_k \subseteq \Hcal$ with non-empty intersection.
Consider the following problem:
\begin{equation} \label{eq:k-projection}
  \begin{aligned}
    & \underset{h \in \Hcal}{\text{miminize}}
      & & \tfrac12 \|h - g\|^2
          \,, \\
    & \text{s.t.}
      & & h \in \bigcap_{k=1}^K C_k
          \,.
  \end{aligned}
\end{equation}
Dykstra's algorithm assumes that it is cheap an easy to project onto $C_k$ for any $k$,
and proposes the following iterations: starting with $\delta^0_k = 0$ and $g^0_K = g$ do
\begin{align}
  g^t_k
    &= \mathrm{proj}_{C_k}\bigl( g^t_{k-1} + \delta^{t-1}_k \bigr)
    \,, \\
  \delta^t_k
    &= (g^t_{k-1} + \delta^{t-1}_k) - g^t_k
    \,,
    \, k = 1,\,\ldots,\,K
\end{align}
where $g^t_0 = g^{t-1}_K$ and $\mathrm{proj}_{C_k}(g) = \arg\min_{h \in C_k} \tfrac12 \|g - h\|^2$.

The relation $g^t_{k-1} - g^t_k = \delta^t_k - \delta^{t-1}_k$ implies that
\begin{align*}
  % g^t_k - g^t_{k-1} &= \delta^{t-1}_k - \delta^t_k \,, \\
  g^t_k - g^{t-1}_K
    &= g^t_k - g^t_0
    = \sum_{j=1}^k \delta^{t-1}_j - \delta^t_j
    \,, \\
  % g^t_K - g^{t-1}_K
  %   &= \sum_{j=1}^K \delta^{t-1}_j - \delta^t_j
  %   \,, \\
  g^t_K - g^0_K
    &= \sum_{s=1}^t g^s_K - g^{s-1}_K
    % = \sum_{s=1}^t \sum_{j=1}^K \delta^{s-1}_j - \delta^s_j
    = \sum_{j=1}^K \sum_{s=1}^t \delta^{s-1}_j - \delta^s_j
    % = \sum_{j=1}^K \delta^0_j - \delta^t_j
    = - \sum_{j=1}^K \delta^t_j
    \,, \\
  g^t_k
    &= g^0_K - \sum_{j=1}^K \delta^{t-1}_j + \sum_{j=1}^k \delta^{t-1}_j - \delta^t_j
    = g - \sum_{j=1}^k \delta^t_j - \sum_{j=k+1}^K \delta^{t-1}_j
    \,. 
\end{align*}
Therefore
\begin{equation*}
  g^t_k
    = \mathrm{proj}_{C_k}\bigl(
      g^t_{k-1} + \delta^{t-1}_k
    \bigr)
    %= g - \sum_{j=1}^{k-1} \delta^t_j - \sum_{j=k}^K \delta^{t-1}_j + \delta^{t-1}_k
    = \mathrm{proj}_{C_k}\biggl(
      g - \sum_{j=1}^{k-1} \delta^t_j - \sum_{j=k+1}^K \delta^{t-1}_j
    \biggr)
    \,.
\end{equation*}

This relation also implies that the difference $\|g^t_{k-1} - g^*\|^2 - \|g^t_k - g^*\|^2$ is
\begin{align*}
  \ldots
    &= 2 \langle \delta^t_k - \delta^{t-1}_k, g^t_k - g^* \rangle
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \\
    &= 2 \langle \delta^t_k, g^t_k - g^* \rangle
      - 2 \langle \delta^{t-1}_k, g^t_k - g^* \rangle
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \\
    &= \underbrace{2 \langle \delta^t_k, g^t_k - g^* \rangle}_{b^t_k}
      - \underbrace{2 \langle \delta^{t-1}_k, g^{t-1}_k - g^* \rangle}_{b^{t-1}_k}
      + \underbrace{2 \langle \delta^{t-1}_k, g^{t-1}_k - g^t_k\rangle}_{a^t_k}
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \,.
\end{align*}
Therefore
\begin{equation*}
  % \|g^t_{k-1} - g^*\|^2 - \|g^t_k - g^*\|^2
  %   &= (b^t_k - b^{t-1}_k) + a^t_k + \| \delta^t_k - \delta^{t-1}_k \|^2
  %   \,, \\
  \|g^t_0 - g^*\|^2 - \|g^t_k - g^*\|^2
    = \sum_{j=1}^k (b^t_j - b^{t-1}_j)
      + \sum_{j=1}^k a^t_j + \| \delta^t_j - \delta^{t-1}_j \|^2
    \,,
\end{equation*}
and $\|g^0_K - g^*\|^2 - \|g^t_K - g^*\|^2 = \sum_{s=1}^t \|g^{s-1}_K - g^*\|^2 - \|g^s_K - g^*\|^2$
is given by
\begin{align*}
  \dots
    &= \sum_{s=1}^t \sum_{j=1}^K (b^s_j - b^{s-1}_j)
      + \sum_{s=1}^t \sum_{j=1}^K a^s_j + \| \delta^s_j - \delta^{s-1}_j \|^2
    %   \\
    % &= \sum_{j=1}^K b^t_j - b^0_j
    %   + \sum_{s=1}^t \sum_{j=1}^K a^s_j
    %   + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
      \\
    &= \sum_{j=1}^K b^t_j
      + \sum_{s=1}^{t-1} \sum_{j=1}^K a^{s+1}_j
      + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
    \,,
\end{align*}
since $b^0_j = 2 \langle \delta^0_k, g^0_k - g^* \rangle = 0$ and $a^1_j = 2 \langle
\delta^0_j, g^0_j - g^1_j\rangle = 0$. Furthermore, $b^s_j = 2 \langle \delta^s_j,
g^s_j - g^* \rangle \geq 0$ and $a^{s+1}_j = 2 \langle \delta^s_j, g^s_j - g^{s+1}_j\rangle
\geq 0$ since $g^*\in C_j$, $g^s_j \in C_j$ and $\delta^s_j = (g^s_{j-1} + \delta^{s-1}_j)
- g^s_j$ for all $s$ and $j$ from the property of the projection $g^s_j$ of $g^s_{j-1}
+ \delta^{s-1}_j$ onto $C_j$.

Finally we have the bound for all $t$:
\begin{align*}
  \|g^0_K - g^*\|^2
    &= \|g^t_K - g^*\|^2
      + \sum_{j=1}^K b^t_j
      + \sum_{s=1}^{t-1} \sum_{j=1}^K a^{s+1}_j
      + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
      \\
    &\geq \|g^t_K - g^*\|^2
      + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
      \\
    % \delta^s_j - g^s_{j-1} + g^s_j = \delta^{s-1}_j
    % \,,
\end{align*}


\paragraph{Application to the GLS estimate} % (fold)
\label{par:application_to_the_gls_estimate}

The GLS estimate of $y\sim X$ with weights $\Omega$ is $\hat{\beta} = \bigl(X^\T \Omega^{-1} X
\bigr)^{-1} X^\T \Omega^{-1} y$ and solves the unconstrained problem
\begin{equation} \label{eq:gls_unc}
  \begin{aligned}
    & \underset{\beta\in \real^p}{\text{miminize}}
      & & \tfrac12 \bigl(y - X \beta\bigr)^\T \Omega^{-1} \bigl(y - X \beta\bigr)
          \,.
  \end{aligned}
\end{equation}

Noting that the objective can be rewritten as
\begin{align*}
  \bigl(y - X \beta\bigr)^\T \Omega^{-1} \bigl(y - X \beta\bigr)
    &= \bigl(y - X \hat{\beta}\bigr)^\T \Omega^{-1} \bigl(y - X \hat{\beta}\bigr)
    \\
    &+ 2 \bigl(y - X \hat{\beta}\bigr)^\T \Omega^{-1} X \bigl(\hat{\beta} - \beta\bigr)
    \\
    &+ \bigl(X \hat{\beta} - X \beta\bigr)^\T \Omega^{-1} \bigl(X \hat{\beta} - X \beta\bigr)
    \,.
\end{align*}
and that
\begin{align*}
  \bigl(y - X \hat{\beta}\bigr)^\T
    &= \bigl(y - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \Omega^{-1} y\bigr)^\T \Omega^{-1} X
    \\
    &= y^\T \Omega^{-\T} \bigl(\Omega - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \bigr)^\T \Omega^{-1} X
    \\
    &= y^\T \Omega^{-1} \bigl(X - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \Omega^{-1} X \bigr)
    = y^\T \Omega^{-1} \bigl(X - X\bigr)
    \,,
\end{align*}
we get a reformulation of the constrained GLS problem
\begin{equation} \label{eq:gls_proj}
  \begin{aligned}
    & \underset{\beta\in C}{\text{miminize}}
      & & \tfrac12 \bigl(\beta - \hat{\beta} \bigr)^\T X^\T \Omega^{-1} X \bigl(\beta - \hat{\beta}\bigr)
          \,,
  \end{aligned}
\end{equation}
as a projection of $\hat{\beta}$ onto $C$ with respect a special metric.

% paragraph application_to_the_gls_estimate (end)

\end{document}
