\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage[mathcal]{euscript}

\usepackage{url}

\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\T}{\mathrm{T}}

\title{Some notes on Boyle, Dykstra (1986)}
\author{Nazarov Ivan}

\date{\today}

\begin{document}
\maketitle

Let $(\Hcal, \langle\cdot,\cdot\rangle)$ be a Hilbert space. We say that $(x_n)_{n\geq1} 
\in \Hcal$ converges {\bf strongly} to $x\in\Hcal$ if $\|x_n - x\| \to 0$. A sequence 
converges to $x$ weakly, if $\langle x_n, z \rangle \to \langle x, z\rangle$ for all
$z\in \Hcal$. Due to continuity of $\langle\cdot,\cdot\rangle$ w.r.t the product norm
topology and $\real$, strong convergence implies weak convergence.

A property of sequences, crucial for optimization problems in finite-dimensional spaces,
is strong sequential compactness: that every bounded sequence has a strongly convergent
subsequence (Bolzano-Weierstrass theorem). But in finite dimensional spaces it fails to
hold.

\subsection*{Projection onto one convex set} % (fold)
\label{sub:projection_onto_one_convex_set}

Let $C \subseteq \Hcal$ be a non-empty closed convex subset of $\Hcal$. Then for any
$g\in \Hcal$ there exists $g^*\in C$ such that $\|g - g^*\| = \inf_{h\in C} \|g - h\|$.
The element $g^*$ is a projection if and only if $\langle g - g^*, g^* - h\rangle
\geq 0$ for all $h \in C$. This follows from the minimization problem and the convexity
of $C$.

Indeed, let $f_t = g^* + t (h - g ^*)$ for any $t \in [0, 1]$. Then we have $f_t \in C$
and $\|f_t - g\| \geq \|g^* - g\|$ from the optimality of $g^*$. However
\begin{align*}
  \|f_t - g \pm g^*\|^2 - \|g^* - g\|^2
    &= \|f_t - g^*\|^2 + 2 \langle f_t - g^*, g^* - g\rangle
    \\
    &= t^2 \|h - g^*\|^2 + 2 t \langle h - g^*, g^* - g\rangle
    \,,
\end{align*}
whence
\begin{equation} \label{eq:proj_prop}
  \langle h - g^*, g^* - g\rangle
    = \lim_{t \downarrow 0}
      \tfrac1{2 t} \bigl( t^2 \|h - g^*\|^2 + 2 t \langle h - g^*, g^* - g\rangle \bigr)
      \geq 0
      \,.
\end{equation}
Conversely, such $g^*\in C$ is a minimizer of $\|h - g\|$ for $h \in C$:
\begin{equation*}
  \|h - g\|^2 - \|g - g^*\|^2
    = \|h - g^*\|^2 + 2 \langle g^* - h, g - g^*\rangle
    \geq \|h - g^*\|^2 \geq 0
    \,.
\end{equation*}

% subsection* projection_onto_one_convex_set (end)

\subsection*{Projection onto and intersection of convex sets} % (fold)
\label{sub:projection_onto_and_intersection_of_convex_sets}

Let $K$ non-empty closed convex sets $C_k \subseteq \Hcal$ with non-empty intersection.
Consider the following problem:
\begin{equation} \label{eq:k-projection}  % \tag{Proj}
  \begin{aligned}
    & \underset{h \in \Hcal}{\text{minimize}}
      & & \tfrac12 \|h - g\|^2
          \,, \\
    & \text{s.t.}
      & & h \in C = \bigcap_{k=1}^K C_k
          \,.
  \end{aligned}
\end{equation}
Dykstra's algorithm assumes that it is cheap an easy to project onto $C_k$ for any $k$,
and proposes the following iterations: {\bf starting} with $\delta^0_k = 0$ and $g^0_K = g$
{\bf do}
\begin{equation} \label{eq:dykstra_iter}  % \tag{}
  \begin{aligned}
    g^t_k
      &= \mathrm{proj}_{C_k}\bigl( g^t_{k-1} + \delta^{t-1}_k \bigr)
      \,,
      \, g^t_0 = g^{t-1}_K
      \,, \\
    \delta^t_k
      &= (g^t_{k-1} + \delta^{t-1}_k) - g^t_k
      \,,
      \, k = 1\ldots K
  \end{aligned}
\end{equation}
where $\mathrm{proj}_{C_k}(g) = \arg\min_{h \in C_k} \tfrac12 \|g - h\|^2$. Note
that, since $g^t_k$ is the projection of $g^t_{k-1} + \delta^{t-1}_k$ onto $C_k$,
the projection property \eqref{eq:proj_prop} implies that $\delta^t_k$ in
\eqref{eq:dykstra_iter} satisfy
\begin{equation} \label{eq:dykstra_prop}
  \forall{h\in C_k}
  \,\,
  \langle \delta^t_k, g^t_k - h \rangle
  = \bigl\langle (g^t_{k-1} + \delta^{t-1}_k) - g^t_k, g^t_k - h \bigr \rangle
  \geq 0
  \,.
\end{equation}
In particular, $h$ can be $g^s_k \in C_k$ for any $s\geq 1$ or any element of
$C = \bigcap_{j=1}^K C_j \subseteq C_k$.

Below we shall show that iterations \eqref{eq:dykstra_iter} converge to $g^*$, the
solution of \eqref{eq:k-projection}.

% subsection* projection_onto_and_intersection_of_convex_sets (end)


\subsection*{Working with the iterations} % (fold)
\label{sub:working_with_the_iterations}

The proof hinges on a key ``balance'' property of the updates in \eqref{eq:dykstra_iter}
and the property of the projection \eqref{eq:proj_prop}. The relation $g^t_{k-1} - g^t_k =
\delta^t_k - \delta^{t-1}_k$ implies
\begin{align}
  % g^t_k - g^t_{k-1} &= \delta^{t-1}_k - \delta^t_k
  %   \,, \notag \\
  g^t_k - g^{t-1}_K
    &= g^t_k - g^t_0
      = \sum_{j=1}^k (\delta^{t-1}_j - \delta^t_j)
    \,, \notag \\
  % g^t_K - g^{t-1}_K
  %   &= \sum_{j=1}^K \delta^{t-1}_j - \delta^t_j
  %   \,, \notag \\
  g^t_K - g^0_K
    &= \sum_{s=1}^t (g^s_K - g^{s-1}_K)
    % = \sum_{s=1}^t \sum_{j=1}^K \delta^{s-1}_j - \delta^s_j
    = \sum_{j=1}^K \sum_{s=1}^t (\delta^{s-1}_j - \delta^s_j)
    % = \sum_{j=1}^K \delta^0_j - \delta^t_j
    = \sum_{j=1}^K \delta^0_j - \sum_{j=1}^K \delta^t_j
    \,, \label{eq:tK_iterate} \\
  g^t_k - g
    &= g^t_k - g^0_K
    = - \sum_{j=1}^K \delta^{t-1}_j + \sum_{j=1}^k \delta^{t-1}_j - \delta^t_j
    = - \sum_{j=1}^k \delta^t_j - \sum_{j=k+1}^K \delta^{t-1}_j
    \,. \notag
\end{align}
This relation also implies that the difference $\|g^t_{k-1} - g^*\|^2 - \|g^t_k - g^*\|^2
= \ldots$ is
\begin{align*}
  \ldots
    % &= 2 \langle g^t_{k-1} - g^t_k, g^t_k - g^* \rangle
    %   + \| \delta^t_k - \delta^{t-1}_k \|^2
    &= 2 \langle \delta^t_k - \delta^{t-1}_k, g^t_k - g^* \rangle
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \\
    &= 2 \langle \delta^t_k, g^t_k - g^* \rangle
      - 2 \langle \delta^{t-1}_k, g^t_k - g^* \rangle
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \\
    &= \underbrace{2 \langle \delta^t_k, g^t_k - g^* \rangle}_{b^t_k}
      - \underbrace{2 \langle \delta^{t-1}_k, g^{t-1}_k - g^* \rangle}_{b^{t-1}_k}
      + \underbrace{2 \langle \delta^{t-1}_k, g^{t-1}_k - g^t_k\rangle}_{a^{t-1}_k}
      + \underbrace{\| \delta^t_k - \delta^{t-1}_k \|^2}_{c^t_k}
    \,,
\end{align*}
with $b^0_k = 2 \langle \delta^0_k, g^0_k - g^* \rangle = 0$ and $a^0_k = 2 \langle
\delta^0_k, g^0_k - g^1_k\rangle = 0$. Furthermore
\begin{align}
  % \|g^t_{k-1} - g^*\|^2 - \|g^t_k - g^*\|^2
  %   &= (b^t_k - b^{t-1}_k) + a^{t-1}_k + c^t_k
  %   \,, \notag \\
  \|g^t_0 - g^*\|^2 - \|g^t_k - g^*\|^2
    &= \sum_{j=1}^k (b^t_j - b^{t-1}_j)
      + \sum_{j=1}^k \bigl( a^{t-1}_j + c^t_j \bigr)
    \,, \label{eq:t0k_norm} \\
  \bigl \| g^{t_1}_K - g^* \bigr \|^2 - \bigl \| g^{t_2}_K - g^* \bigr \|^2
    % &= \sum_{s=t_1+1}^{t_2} \|g^{s-1}_K - g^*\|^2 - \|g^s_K - g^*\|^2
    %   \notag \\
    &= \sum_{t=t_1+1}^{t_2} \Bigl(
        \sum_{j=1}^K (b^t_j - b^{t-1}_j)
          + \sum_{j=1}^K \bigl( a^{t-1}_j + c^t_j \bigr)
        \Bigr)
      \notag \\
    % &= \sum_{j=1}^K \sum_{t=t_1+1}^{t_2} (b^t_j - b^{t-1}_j)
    %     + \sum_{j=1}^K \sum_{t=t_1+1}^{t_2} a^{t-1}_j
    %     + \sum_{t=t_1+1}^{t_2} \sum_{j=1}^K c^t_j
    %   \notag \\
    &= \sum_{j=1}^K (b^{t_2}_j - b^{t_1}_j)
        + \sum_{j=1}^K \sum_{t=t_1}^{t_2-1} a^t_j
        + \sum_{t=t_1+1}^{t_2} \sum_{j=1}^K c^t_j
      \notag \\
    &= \sum_{j=1}^K \Bigl( b^{t_2}_j + a^{t_1}_j - b^{t_1}_j
          + \sum_{t=t_1+1}^{t_2-1} a^t_j + \sum_{t=t_1+1}^{t_2} c^t_j
      \Bigr)
    \,, \label{eq:t1t2K_norm}
\end{align}
For $t_1 = 0$ and $t_2 = t$, we get
\begin{align*}
  \|g^0_K - g^*\|^2 - \|g^t_K - g^*\|^2
    &= \sum_{k=1}^K b^t_k
        + \sum_{k=1}^K \sum_{s=1}^{t-1} a^s_k
        + \sum_{s=1}^t \sum_{k=1}^K \| \delta^s_k - \delta^{s-1}_k \|^2
      \\
    &\geq
      \sum_{s=1}^t \sum_{k=1}^K \| \delta^s_k - \delta^{s-1}_k \|^2
    = \sum_{s=1}^t \sum_{k=1}^K \| g^s_{k-1} - g^s_k \|^2
    \,,
\end{align*}
since $b^t_k \geq 0$ and $a^t_k \geq 0$ for all $t\geq 0$ by \eqref{eq:dykstra_prop}.
The first and the last lines imply that $\|g^0_K - g^*\| \geq \|g^t_k - g^*\|$, whence
$(g^t_k)_{t\geq1}$ is bounded for all $k$.

% subsection* working_with_the_iterations (end)

\subsection*{Limiting properties} % (fold)
\label{sub:limiting_properties}

This section follows the proof in \cite{boyledykstra1986}.

The partial sum in the hand side is bounded, and therefore the limit exists and the
series $\sum_{t\geq 1} \sum_{k=1}^K \| g^t_{k-1} - g^t_k \|^2$ converge. This implies
that for any $k$ we have $\|g^t_{k-1} - g^t_k\|^2 \to 0$ as $t\to \infty$.
Furthermore, if $(g^t_k)_{t\geq 1}\in \Hcal$ converges strongly to $h$, then so does
$(g^t_p)_{t\geq 1}$ for any $p=1,\,\ldots,\,K$: indeed
\begin{equation*}
  \limsup_{t\to \infty} \| g^t_p - h \|
    \leq \limsup_{t\to \infty} \|g^t_k - h\|
      + \sum_{j=p+1}^k \limsup_{t\to \infty} \| g^t_{j-1} - g^t_j \|
    \,.
\end{equation*}
Therefore proving the convergence of $(g^t_K)_{t\geq 1}$ is sufficient.

For any $f\in C = \bigcap_{k=1}^K C_k$ we have $\langle \delta^t_k,  g^t_k - f \rangle
\geq 0$ by \eqref{eq:dykstra_prop}, whence
\begin{equation*}
  \langle g^t_1 - f, g^t_K - g \rangle
    % = - \sum_{k=1}^K \langle g^t_1 \pm g^t_k - f, \delta^t_k \rangle
    = \sum_{k=1}^K \langle g^t_k - g^t_1, \delta^t_k \rangle
      + (-1) \sum_{k=1}^K \langle g^t_k - f, \delta^t_k \rangle
    \leq \sum_{k=2}^K \langle g^t_k - g^t_1, \delta^t_k \rangle
      \,.
\end{equation*}
If we put $d_t = \sum_{k=2}^K \| g^t_{k-1} - g^t_k \|$, then we get
\begin{align*}
  \sum_{k=2}^K \bigl\lvert \langle g^t_k - g^t_1, \delta^t_k \rangle \bigr\rvert
    &\leq \sum_{k=2}^K \|\delta^t_k\| \| g^t_k - g^t_1 \|
    \leq \sum_{k=2}^K \|\delta^t_k\| \sum_{j=2}^k \| g^t_{j-1} - g^t_j \|
    \\
    &\leq \sum_{k=2}^K \|\delta^t_k\| \, d_t
    = \sum_{k=2}^K d_t \Bigl\| \sum_{s=1}^t (\delta^s_k - \delta^{s-1}_k) \Bigr\|
    \\
    &\leq \sum_{k=2}^K \sum_{s=1}^t d_t \|\delta^s_k - \delta^{s-1}_k \|
    = \sum_{s=1}^t \sum_{k=2}^K d_t \|g^s_{k-1} - g^s_k \|
    = \sum_{s=1}^t d_t d_s
      \,.
\end{align*}
Observe that by Cauchy-Schwartz (H{\"o}lder for $p=q=2$) inequality we have
\begin{equation*}
  d_t
    \leq \sum_{k=1}^K 1 \cdot \| g^t_{k-1} - g^t_k \|
    \leq K^{\tfrac12} \biggl(
      \sum_{k=1}^K \| g^t_{k-1} - g^t_k \|^2
    \biggr)^{\tfrac12}
    \,,
\end{equation*}
which implies that
\begin{equation*}
  \sum_{t\geq 1} d^2_t
    \leq K \sum_{t\geq 1} \sum_{k=1}^K \| g^t_{k-1} - g^t_k \|^2
      < +\infty
    \,.
\end{equation*}
For such series there is $(t_n)_{n\geq1} \uparrow$ such that $\sum_{s=1}^{t_n} d_s
d_{t_n} \to 0$ ({\bf \color{red} WHY?}). Hence
\begin{equation*}
  \lim_{n \to \infty} \langle g^{t_n}_1 - f, g^{t_n}_K - g \rangle
    \leq \lim_{n \to \infty} \sum_{s=1}^{t_n} d_s d_{t_n}
    = 0
    \,.
\end{equation*}

Since $\|g^t_{k-1} - g^t_k \| \to 0$ as $t\to \infty$ and $(g^t_k)_{t\geq1}$ is
bounded for all $k$, we have
\begin{align*}
  \langle g^{t_n}_K - f, g^{t_n}_K - g \rangle
    &= \Bigl(
      \langle g^{t_n}_1 - f, g^{t_n}_K - g \rangle
        + \sum_{j=2}^K \langle g^{t_n}_j - g^{t_n}_{j-1}, g^{t_n}_K - g \rangle
      \Bigr)
      \\
    &\leq \langle g^{t_n}_1 - f, g^{t_n}_K - g \rangle
      + \bigl(
        \|g\| + \sup_{t,k\geq 1} \|g^t_k\|
      \bigr) \sum_{j=2}^K \|g^{t_n}_j - g^{t_n}_{j-1}\|
      \,.
\end{align*}
Hence the right-hand side not greater than zero in the limit.

Next, since $\|g^{t_n}_k\|$ is bounded, it has a convergent subsequence in $\real$.
We can assume that $t_n$ itself is this subsequence, since refining convergent
sequences does not affect its limiting properties.

Finally, by weak compactness the bounded sequence $(g^{t_n}_K)_{n\geq 1}$ has another
subsequence that weakly converges to some $h \in \Hcal$. Again, we are free to let
$(t_n)_{n\geq 1}$ be that subsequence.

% So far we have shown that the iterates $g^t_k$ have the following properties:
% \begin{itemize}
%   \item $(g^t_k)_{t\geq 1}$ is bounded for all $k$;
%   \item if $(g^t_K)_{t\geq 1}$ converges strongly then so do $(g^t_k)_{\geq t}$ for $k=1..K$;
%   \item there is $(t_n)_{n\geq1} \uparrow$ with $\lim_{n\to 0} \langle g^{t_n}_K - f,
%   g^{t_n}_K - g \rangle \leq 0$ for all $f\in C$.
% \end{itemize}

% subsection* limiting_properties (end)

\subsection*{Finding a convergent sequence} % (fold)
\label{sub:finding_a_convergent_sequence}

We have shown that the iterates $g^t_k$ of \eqref{eq:dykstra_iter} admit
$(t_n)_{n \geq 1} \uparrow$ such that
\begin{itemize}
  \item there is $h\in \Hcal$ with $g^{t_n}_K \rightharpoonup h$;
  \item there exists $L \in \real$ with $\|g^{t_n}_K\| \to L$;
  \item $\lim_{n\to \infty} \langle g^{t_n}_K - f, g^{t_n}_K - g \rangle \leq 0$
  for all $f\in C = \bigcap_{k=1}^K C_k$.
\end{itemize}
Observe also, that if $x_n \rightharpoonup x$ and $\|x_n\| \to L$, then from
\begin{equation*}
  \|x\|^2
    = \limsup_{n\to \infty} \langle x_n, x\rangle
    \leq \limsup_{n\to \infty} \|x_n\| \|x\|
    = L \|x\|
    \,,
\end{equation*}
we conclude that $\|x\| \leq L$. We apply this to $x_n = g^{t_n}_K$ and $x = h$ to
get $\|h\|\leq L$.

From the properties of $(t_n)_{n\geq1}$ we have
\begin{align} \label{eq:weak_lim_equ}
  \lim_{n\to \infty}
    \langle g^{t_n}_K - f, g^{t_n}_K - g \rangle
    &= \lim_{n\to \infty} \|g^{t_n}_K\|^2
      - \lim_{n\to \infty} \langle g^{t_n}_K, g \rangle
      - \lim_{n\to \infty} \langle f, g^{t_n}_K \rangle
      + \langle f, g \rangle
    \notag \\
    &= L^2 \pm \|h\|^2 - \langle h, g \rangle - \langle h, g \rangle
      + \langle f, g \rangle
    \notag \\
    &= L^2 - \|h\|^2 + \langle h - f, h - g \rangle
  \,,
\end{align}
and thus $\langle h - f, h - g \rangle \leq 0$ for any $f \in C$.

Next, for any $f\in \Hcal$ and $k=1..K$ we have the following inequality
\begin{equation*}
  \limsup_{n\to\infty}\,
    \bigl\lvert
      \langle g^{t_n}_K, f \rangle - \langle g^{t_n}_k, f \rangle
    \bigr\rvert
    \leq
      \|f\| \sum_{j=k+1}^K \limsup_{n\to\infty}
        \bigl \|g^{t_n}_{j-1} - g^{t_n}_j \bigr \|
      \,,
\end{equation*}
which implies that $g^{t_n}_k \rightharpoonup h$ for any $k$. However, for any $k$
from the weak convergence of $x_n = g^{t_n}_k$ to $h$ it is possible to deduce that
there is a subsequence $(n^k_i)_{i\geq1}$ with $\tfrac1m \sum_{i=1}^m x_{n^k_i} \to h$
strongly ({\bf \color{red} WHY?}). Therefore $h \in C_k$, since $x_{n^k_i} \in C_k$
for all $i$, and $C_k$ is convex and {\bf strongly closed}.

Therefore, the weak limit of $g^{t_n}_k$ is in $C$. From \eqref{eq:weak_lim_equ}
for $f = h \in C$ we conclude that $0 \geq L^2 - \|h\|^2$, whence $\|h\| = L$.
This implies that $g^{t_n}_k \to h$, because
\begin{equation*}
  \limsup_{n\to\infty} \| g^{t_n}_k - h \|^2
    = \| h \|^2 + \limsup_{n\to\infty} \|g^{t_n}_k\|^2 - 2\langle g^{t_n}_k, h \rangle
    = \| h \|^2 + L^2 - 2 \| h \|^2
    \,.
\end{equation*}
To conclude, we have found a subsequence $(t_n)_{n\geq1} \uparrow$ such that
\begin{itemize}
  \item $g^{t_n}_k \to h$ strongly for any $k$ and $h=g^*$, since $h \in C$ and
  satisfies \eqref{eq:proj_prop}.
\end{itemize}

% subsection* finding_a_convergent_sequence (end)

\subsection*{Back to the iterations} % (fold)
\label{sub:back_to_the_iterations}

From \eqref{eq:t1t2K_norm} for any $m \geq 1$ we get
\begin{align*}
  \bigl \| g^{t_n}_K - g^* \bigr \|^2 - \bigl \| g^{t_n+m}_K - g^* \bigr \|^2
    &= \sum_{j=1}^K (b^{t_n+m}_j - b^{t_n}_j)
        + \sum_{j=1}^K \Bigl(
          \sum_{i=0}^{m-1} a^{t_n+i}_j
          + \sum_{i=1}^m c^{t_n+i}_j
      \Bigr)
      \\
    &\geq - \sum_{j=1}^K b^{t_n}_j
      = 2 \sum_{j=1}^K \langle \delta^{t_n}_k, g^* - g^{t_n}_k \rangle
    \,,
\end{align*}
Since \eqref{eq:tK_iterate} implies that $g - g^t_K = \sum_{k=1}^K \delta^t_k$, we
can get the following upper bound
\begin{align*}
  \Bigl \lvert \sum_{k=1}^K \langle \delta^t_k, g^* - g^t_k \rangle \Bigr \rvert
    &= \Bigl \lvert
      \Bigl\langle \sum_{k=1}^K \delta^t_k, g^* - g^t_1 \Bigr \rangle
      + \sum_{k=1}^K \langle \delta^t_k, g^t_1 - g^t_k \rangle
      \Bigr \rvert
      \\
    &\leq \bigl\lvert \langle g^t_K - g, g^t_1 - g^* \rangle \bigr\rvert 
      + \sum_{k=2}^K \bigl\lvert \langle \delta^t_k, g^t_k - g^t_1 \rangle \bigr\rvert 
      \\
    &\leq \bigl\lvert \langle g^t_K - g, g^t_1 - g^* \rangle \bigr\rvert 
      + \sum_{s=1}^t d_t d_s
      \,.
\end{align*}
Applying this bound to the subsequence $(t_n)_{n\geq1}$ we get in the limit
\begin{align*}
  \lim_{n\to \infty}
    \Bigl \lvert \sum_{k=1}^K \langle \delta^{t_n}_k, g^* - g^{t_n}_k \rangle \Bigr \rvert
    &\leq \lim_{n\to \infty}
      \bigl\lvert \langle g^{t_n}_K - g, g^{t_n}_1 - g^* \rangle \bigr\rvert
      + \lim_{n\to \infty} \sum_{s=1}^{t_n} d_s d_{t_n}
      \\
    &\leq \lim_{n\to \infty} \|g^{t_n}_K - g\| \|g^{t_n}_1 - g^* \|
      = 0
    \,,
\end{align*}
because $g^{t_n}_k \to g^*$ and $g^t_k$ is bounded. Therefore 
\begin{equation*}
  \limsup_{n\to\infty} \bigl \| g^{t_n+m}_K - g^* \bigr \|^2
    \leq \limsup_{n\to\infty} \bigl \| g^{t_n}_K - g^* \bigr \|^2
      + 2 \Bigl \lvert \sum_{k=1}^K \langle \delta^{t_n}_k, g^* - g^{t_n}_k \rangle \Bigr \rvert
    \leq 0
    \,.
\end{equation*}

\begin{equation*}
  g^t_k
    = \mathrm{proj}_{C_k}\bigl(
      g^t_{k-1} + \delta^{t-1}_k
    \bigr)
    %= g - \sum_{j=1}^{k-1} \delta^t_j - \sum_{j=k}^K \delta^{t-1}_j + \delta^{t-1}_k
    = \mathrm{proj}_{C_k}\biggl(
      g - \sum_{j=1}^{k-1} \delta^t_j - \sum_{j=k+1}^K \delta^{t-1}_j
    \biggr)
    \,.
\end{equation*}

% subsection* back_to_the_iterations (end)


\paragraph{Application to the GLS estimate} % (fold)
\label{par:application_to_the_gls_estimate}

The GLS estimate of $y\sim X$ with weights $\Omega$ is $\hat{\beta} = \bigl(X^\T \Omega^{-1} X
\bigr)^{-1} X^\T \Omega^{-1} y$ and solves the unconstrained problem
\begin{equation} \label{eq:gls_unc}
  \begin{aligned}
    & \underset{\beta\in \real^p}{\text{minimize}}
      & & \tfrac12 \bigl(y - X \beta\bigr)^\T \Omega^{-1} \bigl(y - X \beta\bigr)
          \,.
  \end{aligned}
\end{equation}

Noting that the objective can be rewritten as
\begin{align*}
  \bigl(y - X \beta\bigr)^\T \Omega^{-1} \bigl(y - X \beta\bigr)
    &= \bigl(y - X \hat{\beta}\bigr)^\T \Omega^{-1} \bigl(y - X \hat{\beta}\bigr)
    \\
    &+ 2 \bigl(y - X \hat{\beta}\bigr)^\T \Omega^{-1} X \bigl(\hat{\beta} - \beta\bigr)
    \\
    &+ \bigl(X \hat{\beta} - X \beta\bigr)^\T \Omega^{-1} \bigl(X \hat{\beta} - X \beta\bigr)
    \,.
\end{align*}
and that
\begin{align*}
  \bigl(y - X \hat{\beta}\bigr)^\T
    &= \bigl(y - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \Omega^{-1} y\bigr)^\T \Omega^{-1} X
    \\
    &= y^\T \Omega^{-\T} \bigl(\Omega - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \bigr)^\T \Omega^{-1} X
    \\
    &= y^\T \Omega^{-1} \bigl(X - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \Omega^{-1} X \bigr)
    = y^\T \Omega^{-1} \bigl(X - X\bigr)
    \,,
\end{align*}
we get a reformulation of the constrained GLS problem
\begin{equation} \label{eq:gls_proj}
  \begin{aligned}
    & \underset{\beta\in C}{\text{minimize}}
      & & \tfrac12 \bigl(\beta - \hat{\beta} \bigr)^\T X^\T \Omega^{-1} X \bigl(\beta - \hat{\beta}\bigr)
          \,,
  \end{aligned}
\end{equation}
as a projection of $\hat{\beta}$ onto $C$ with respect a special metric.

% paragraph application_to_the_gls_estimate (end)

\end{document}
