\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage[mathcal]{euscript}

\usepackage{url}

\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\T}{\mathrm{T}}

\title{Some notes on Boyle, Dykstra (1986)}
\author{Nazarov Ivan}

\date{\today}

\begin{document}
\maketitle

Let $(\Hcal, \langle\cdot,\cdot\rangle)$ be a Hilbert space. We say that $(x_n)_{n\geq1} 
\in \Hcal$ converges {\bf strongly} to $x\in\Hcal$ if $\|x_n - x\| \to 0$. A sequence 
converges to $x$ weakly, if $\langle x_n, z \rangle \to \langle x, z\rangle$ for all
$z\in \Hcal$. Due to continuity of $\langle\cdot,\cdot\rangle$ w.r.t the product norm
topology and $\real$, strong convergence implies weak convergence.

A property of sequences, crucial for optimization problems in finite-dimensional spaces,
is strong sequential compactness: that every bounded sequence has a stringly convergent
subsequence (Bolzano-Weierstrass theorem). But in finite dimensional spaces it fails to
hold.

Let $C \subseteq \Hcal$ be a non-empty closed convex subset of $\Hcal$. Then for any
$g\in \Hcal$ there exists $g^*\in C$ such that $\|g - g^*\| = \inf_{h\in C} \|g - h\|$,
and for any $h\in C$ we have $\langle g - g^*, g^* - h\rangle \geq 0$. The latter follows
from convexity of $C$ and differentiability: in the following let $f = g^* + t (h -g ^*)$
and observe that $f \in C$ and it we have $\|f - g\| \geq \|g^* - g\|$ for all $t\in
[0, 1]$ (and in the limit $t\downarrow 0$)
\begin{equation*}
  \|f - g\|^2
    = \|f - g^*\|^2 + 2 \langle f - g^*, g^* - g\rangle + \|g^* - g\|^2
    % \geq \|f - g^*\|^2 + \|g^* - g\|^2
    \,.
\end{equation*}

Let $K$ non-empty closed convex sets $C_k \subseteq \Hcal$ with non-empty intersection.
Consider the following problem:
\begin{equation} \label{eq:k-projection}
  \begin{aligned}
    & \underset{h \in \Hcal}{\text{miminize}}
      & & \tfrac12 \|h - g\|^2
          \,, \\
    & \text{s.t.}
      & & h \in \bigcap_{k=1}^K C_k
          \,.
  \end{aligned}
\end{equation}
Dykstra's algorithm assumes that it is cheap an easy to project onto $C_k$ for any $k$,
and proposes the following iterations: starting with $\delta^0_k = 0$ and $g^0_K = g$ do
\begin{align}
  g^t_k
    &= \mathrm{proj}_{C_k}\bigl( g^t_{k-1} + \delta^{t-1}_k \bigr)
    \,, \\
  \delta^t_k
    &= (g^t_{k-1} + \delta^{t-1}_k) - g^t_k
    \,,
    \, k = 1,\,\ldots,\,K
\end{align}
where $g^t_0 = g^{t-1}_K$ and $\mathrm{proj}_{C_k}(g) = \arg\min_{h \in C_k} \tfrac12 \|g - h\|^2$.

The relation $g^t_{k-1} - g^t_k = \delta^t_k - \delta^{t-1}_k$ implies that
\begin{align*}
  % g^t_k - g^t_{k-1} &= \delta^{t-1}_k - \delta^t_k \,, \\
  g^t_k - g^{t-1}_K
    &= g^t_k - g^t_0
    = \sum_{j=1}^k \delta^{t-1}_j - \delta^t_j
    \,, \\
  % g^t_K - g^{t-1}_K
  %   &= \sum_{j=1}^K \delta^{t-1}_j - \delta^t_j
  %   \,, \\
  g^t_K - g^0_K
    &= \sum_{s=1}^t g^s_K - g^{s-1}_K
    % = \sum_{s=1}^t \sum_{j=1}^K \delta^{s-1}_j - \delta^s_j
    = \sum_{j=1}^K \sum_{s=1}^t \delta^{s-1}_j - \delta^s_j
    % = \sum_{j=1}^K \delta^0_j - \delta^t_j
    = - \sum_{j=1}^K \delta^t_j
    \,, \\
  g^t_k
    &= g^0_K - \sum_{j=1}^K \delta^{t-1}_j + \sum_{j=1}^k \delta^{t-1}_j - \delta^t_j
    = g - \sum_{j=1}^k \delta^t_j - \sum_{j=k+1}^K \delta^{t-1}_j
    \,. 
\end{align*}
Therefore
\begin{equation*}
  g^t_k
    = \mathrm{proj}_{C_k}\bigl(
      g^t_{k-1} + \delta^{t-1}_k
    \bigr)
    %= g - \sum_{j=1}^{k-1} \delta^t_j - \sum_{j=k}^K \delta^{t-1}_j + \delta^{t-1}_k
    = \mathrm{proj}_{C_k}\biggl(
      g - \sum_{j=1}^{k-1} \delta^t_j - \sum_{j=k+1}^K \delta^{t-1}_j
    \biggr)
    \,.
\end{equation*}

This relation also implies that the difference $\|g^t_{k-1} - g^*\|^2 - \|g^t_k - g^*\|^2$ is
\begin{align*}
  \ldots
    &= 2 \langle \delta^t_k - \delta^{t-1}_k, g^t_k - g^* \rangle
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \\
    &= 2 \langle \delta^t_k, g^t_k - g^* \rangle
      - 2 \langle \delta^{t-1}_k, g^t_k - g^* \rangle
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \\
    &= \underbrace{2 \langle \delta^t_k, g^t_k - g^* \rangle}_{b^t_k}
      - \underbrace{2 \langle \delta^{t-1}_k, g^{t-1}_k - g^* \rangle}_{b^{t-1}_k}
      + \underbrace{2 \langle \delta^{t-1}_k, g^{t-1}_k - g^t_k\rangle}_{a^t_k}
      + \| \delta^t_k - \delta^{t-1}_k \|^2
    \,.
\end{align*}
Therefore
\begin{equation*}
  % \|g^t_{k-1} - g^*\|^2 - \|g^t_k - g^*\|^2
  %   &= (b^t_k - b^{t-1}_k) + a^t_k + \| \delta^t_k - \delta^{t-1}_k \|^2
  %   \,, \\
  \|g^t_0 - g^*\|^2 - \|g^t_k - g^*\|^2
    = \sum_{j=1}^k (b^t_j - b^{t-1}_j)
      + \sum_{j=1}^k a^t_j + \| \delta^t_j - \delta^{t-1}_j \|^2
    \,,
\end{equation*}
and $\|g^0_K - g^*\|^2 - \|g^t_K - g^*\|^2 = \sum_{s=1}^t \|g^{s-1}_K - g^*\|^2 - \|g^s_K - g^*\|^2$
is given by
\begin{align*}
  \dots
    &= \sum_{s=1}^t \sum_{j=1}^K (b^s_j - b^{s-1}_j)
      + \sum_{s=1}^t \sum_{j=1}^K a^s_j + \| \delta^s_j - \delta^{s-1}_j \|^2
    %   \\
    % &= \sum_{j=1}^K b^t_j - b^0_j
    %   + \sum_{s=1}^t \sum_{j=1}^K a^s_j
    %   + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
      \\
    &= \sum_{j=1}^K b^t_j
      + \sum_{s=1}^{t-1} \sum_{j=1}^K a^{s+1}_j
      + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
    \,,
\end{align*}
since $b^0_j = 2 \langle \delta^0_k, g^0_k - g^* \rangle = 0$ and $a^1_j = 2 \langle
\delta^0_j, g^0_j - g^1_j\rangle = 0$. Furthermore, $b^s_j = 2 \langle \delta^s_j,
g^s_j - g^* \rangle \geq 0$ and $a^{s+1}_j = 2 \langle \delta^s_j, g^s_j - g^{s+1}_j\rangle
\geq 0$ since $g^*\in C_j$, $g^s_j \in C_j$ and $\delta^s_j = (g^s_{j-1} + \delta^{s-1}_j)
- g^s_j$ for all $s$ and $j$ from the property of the projection $g^s_j$ of $g^s_{j-1}
+ \delta^{s-1}_j$ onto $C_j$.

Finally we have the bound for all $t$:
\begin{align*}
  \|g^0_K - g^*\|^2
    &= \|g^t_K - g^*\|^2
      + \sum_{j=1}^K b^t_j
      + \sum_{s=1}^{t-1} \sum_{j=1}^K a^{s+1}_j
      + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
      \\
    &\geq \|g^t_K - g^*\|^2
      + \sum_{s=1}^t \sum_{j=1}^K \| \delta^s_j - \delta^{s-1}_j \|^2
    % \delta^s_j - g^s_{j-1} + g^s_j = \delta^{s-1}_j
    \,,
\end{align*}
whence $\sum_{s=1}^t \sum_{j=1}^K \| g^s_{j-1} - g^s_j \|^2 < +\infty$.

This series implies that $\|g^t_{k-1} - g^t_k\|^2 \to 0$ for any $k=1,\,\ldots,\,K$
as $t\to \infty$. If $(g^t_k)_{t\geq 1}\in \Hcal$ converges strongly to $h$, then
so does $(g^t_p)_{t\geq 1}$ for any $p=1,\,\ldots,\,K$:
\begin{equation*}
  \limsup_{t\to \infty} \| g^t_p - h \|
    \leq \limsup_{t\to \infty} \|g^t_k - h\|
      + \sum_{j=p+1}^k \limsup_{t\to \infty} \| g^t_{j-1} - g^t_j \|
    \,.
\end{equation*}
Therefore proving the convergence of $(g^t_K)_{t\geq 1}$ is sufficient.

For any $f\in \bigcap_{k=1}^K C_k$ we have $\langle g^t_K - f, \delta^t_K \rangle
\geq 0$, since $g^t_k$ is a projection onto $C_k$, whence
\begin{equation*}
  \langle g^t_1 - f, g^t_K - g \rangle
    % = - \sum_{k=1}^K \langle g^t_1 \pm g^t_k - f, \delta^t_k \rangle
    = \sum_{k=1}^K \langle g^t_k - g^t_1, \delta^t_k \rangle
      + (-1) \sum_{k=1}^K \langle g^t_k - f, \delta^t_k \rangle
    \leq \sum_{k=2}^K \langle g^t_k - g^t_1, \delta^t_k \rangle
      \,.
\end{equation*}
If we let $a_t = \sum_{j=2}^K \| g^t_{j-1} - g^t_j \|$, then we get
\begin{align*}
  \bigl\lvert \langle g^t_1 - f, g^t_K - g \rangle \bigr\rvert
    &\leq \sum_{k=2}^K \bigl\lvert \langle g^t_k - g^t_1, \delta^t_k \rangle \bigr\rvert
    \leq \sum_{k=2}^K \|\delta^t_k\| \| g^t_k - g^t_1 \|
    \\
    &\leq \sum_{k=2}^K \|\delta^t_k\| \sum_{j=2}^k \| g^t_{j-1} - g^t_j \|
    \leq \sum_{k=2}^K \|\delta^t_k\| \sum_{j=2}^K \| g^t_{j-1} - g^t_j \|
    \\
    &= \sum_{k=2}^K \Bigl\|\sum_{s=1}^t \delta^s_k - \delta^{s-1}_k \Bigr\| a_t
    \leq \sum_{k=2}^K \sum_{s=1}^t \|\delta^s_k - \delta^{s-1}_k \| a_t
    \\
    &= \sum_{s=1}^t \sum_{k=2}^K \|g^s_{k-1} - g^s_k \| a_t
    = \sum_{s=1}^t a_s a_t
      \,.
\end{align*}
Since $2 a b \leq a^2 + b^2$, we have
\begin{align*}
  a^2_t
    &= \sum_{j=2}^K \| g^t_{j-1} - g^t_j \|^2
      + \sum_{j=2}^K \sum_{p > j} 2 \| g^t_{j-1} - g^t_j \| \| g^t_{p-1} - g^t_p \|
    \\
    &\leq \sum_{j=2}^K \| g^t_{j-1} - g^t_j \|^2
        % + \sum_{j=2}^K \sum_{p > j} \sum_{j=2}^K \| g^t_{j-1} - g^t_j \|^2
        + \sum_{j=2}^K \sum_{p > j} \| g^t_{j-1} - g^t_j \|^2 + \| g^t_{p-1} - g^t_p \|^2
    \\
    &\leq \Bigl(1+\tfrac{(K-1)(K-2)}{2}\Bigr) \sum_{j=1}^K \| g^t_{j-1} - g^t_j \|^2
    \,,
\end{align*}
which implies that
\begin{align*}
  \sum_{t\geq 1} a^2_t
    &\leq \Bigl(1+\tfrac{(K-1)(K-2)}{2}\Bigr)
      \sum_{t\geq 1} \sum_{j=1}^K \| g^t_{j-1} - g^t_j \|^2
      < +\infty
    \,,
\end{align*}
Therefore there is $(t_n)_{n\geq1} \uparrow$ such that $\sum_{s=1}^{t_n} a_s a_{t_n}
\to 0$ as $n\longrightarrow \infty$ ({\color{red} WHY?}).
Hence
\begin{equation*}
  \lim_{n \to \infty} \langle g^{t_n}_1 - f, g^{t_n}_K - g \rangle
    \leq \lim_{n \to \infty} \sum_{s=1}^{t_n} a_s a_{t_n}
    = 0
    \,.
\end{equation*}
Since $\|g^t_{k-1} - g^t_k \| \to 0$, $\|g^*-g\|<+\infty$ and $\|g^0_K - g^*\|^2
\geq \|g^t_k - g^*\|^2$ for all $t\geq 1$ and $k=1,\,\ldots,\,K$, we have that
\begin{align*}
  \langle g^{t_n}_K - f, g^{t_n}_K - g \rangle
    &= \langle g^{t_n}_1 - f, g^{t_n}_K - g \rangle
      + \sum_{j=2}^K \langle g^{t_n}_j - g^{t_n}_{j-1}, g^{t_n}_K - g \rangle
      \\
    &\leq \langle g^{t_n}_1 - f, g^{t_n}_K - g \rangle
      + \sum_{j=2}^K \|g^{t_n}_K - g\| \|g^{t_n}_j - g^{t_n}_{j-1}\|
      \,,
\end{align*}
and the $\limsup_{n\to \infty}$ of the right-hand side is zero. Thus there exsits
$(t_n)_{n\geq1} \uparrow$ such that $\lim_{n\to 0} \langle g^{t_n}_K - f, g^{t_n}_K - g
\rangle \leq 0$ for all $f\in \bigcap_{k=1}^K C_k$.

The sequence $(g^{t_n}_K)_{n\geq 1}$ is bounded from the main bound above, whence
by weak compactness it must have another subsequence that weakly converges to some
$h\in \Hcal$. This subsequence can be further refined to make $\|g^t_k\|$ converge
in $\real$. Ultimately, there is a subsequence $(t_n)_{n\geq1}\uparrow$ such that
\begin{itemize}
  \item there is $h\in \Hcal$ such that $g^{t_n}_K \rightharpoonup h$;
  \item $\lim_{n\to \infty} \langle g^{t_n}_K - f, g^{t_n}_K - g \rangle \leq 0$
  for all $f\in \bigcap_{k=1}^K C_k$;
  \item $\lim_{n \to \infty} \|g^{t_n}_K\| \in [0, +\infty)$.
\end{itemize}

\paragraph{Application to the GLS estimate} % (fold)
\label{par:application_to_the_gls_estimate}

The GLS estimate of $y\sim X$ with weights $\Omega$ is $\hat{\beta} = \bigl(X^\T \Omega^{-1} X
\bigr)^{-1} X^\T \Omega^{-1} y$ and solves the unconstrained problem
\begin{equation} \label{eq:gls_unc}
  \begin{aligned}
    & \underset{\beta\in \real^p}{\text{miminize}}
      & & \tfrac12 \bigl(y - X \beta\bigr)^\T \Omega^{-1} \bigl(y - X \beta\bigr)
          \,.
  \end{aligned}
\end{equation}

Noting that the objective can be rewritten as
\begin{align*}
  \bigl(y - X \beta\bigr)^\T \Omega^{-1} \bigl(y - X \beta\bigr)
    &= \bigl(y - X \hat{\beta}\bigr)^\T \Omega^{-1} \bigl(y - X \hat{\beta}\bigr)
    \\
    &+ 2 \bigl(y - X \hat{\beta}\bigr)^\T \Omega^{-1} X \bigl(\hat{\beta} - \beta\bigr)
    \\
    &+ \bigl(X \hat{\beta} - X \beta\bigr)^\T \Omega^{-1} \bigl(X \hat{\beta} - X \beta\bigr)
    \,.
\end{align*}
and that
\begin{align*}
  \bigl(y - X \hat{\beta}\bigr)^\T
    &= \bigl(y - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \Omega^{-1} y\bigr)^\T \Omega^{-1} X
    \\
    &= y^\T \Omega^{-\T} \bigl(\Omega - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \bigr)^\T \Omega^{-1} X
    \\
    &= y^\T \Omega^{-1} \bigl(X - X \bigl(X^\T \Omega^{-1} X \bigr)^{-1} X^\T \Omega^{-1} X \bigr)
    = y^\T \Omega^{-1} \bigl(X - X\bigr)
    \,,
\end{align*}
we get a reformulation of the constrained GLS problem
\begin{equation} \label{eq:gls_proj}
  \begin{aligned}
    & \underset{\beta\in C}{\text{miminize}}
      & & \tfrac12 \bigl(\beta - \hat{\beta} \bigr)^\T X^\T \Omega^{-1} X \bigl(\beta - \hat{\beta}\bigr)
          \,,
  \end{aligned}
\end{equation}
as a projection of $\hat{\beta}$ onto $C$ with respect a special metric.

% paragraph application_to_the_gls_estimate (end)

\end{document}
