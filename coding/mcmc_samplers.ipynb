{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a multi-modal density: we can evaluate it, know its structure,\n",
    "but not the normalizing constant.\n",
    "\n",
    "$$\n",
    "p(\\theta)\n",
    "    = \\tfrac1Z \\mathop{\\mathrm{exp}} \\bigl(\n",
    "        \\log f(\\theta)\n",
    "    \\bigr)\n",
    "    \\,.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a banana distribution:\n",
    "\n",
    "$$\n",
    "p(x)\n",
    "    \\propto p_{\\mathcal{N}(0, 1)} \\circ \\phi(x)\n",
    "    \\,, $$\n",
    "\n",
    "where $\\phi$ is the based on the\n",
    "[Banana](https://en.wikipedia.org/wiki/Rosenbrock_function)\n",
    "function and given by $\n",
    "\\phi\n",
    "\\colon \\mathbb{R}^2 \\to \\mathbb{R}^2\n",
    "\\colon (x, y) \\mapsto (a x, b (y-x^2))\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_banana_base(x, a=0.75, b=1.05):\n",
    "#     phi = torch.stack([a - x[..., 0], b * (x[..., 1] - x[..., 0]**2)], dim=-1)  # a=1.75, b=5\n",
    "    phi = torch.stack([a * (x[..., 0] - x[..., 1]**2), b * (x[..., 1] - x[..., 0]**2)], dim=-1)\n",
    "    return -0.5 * torch.norm(phi, p=2, keepdim=False, dim=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_density(x):\n",
    "    mu = map(torch.tensor, [(2., 2.), (-2., -2.)])\n",
    "    a, b, s = [+0.75, -0.75], [+3.05, -1.05], [+1, -1]\n",
    "\n",
    "    compo = map(lambda m, a, b, s: log_banana_base(s*(x - m), a, b), mu, a, b, s)\n",
    "    stacked = torch.stack([*compo], dim=0)\n",
    "\n",
    "    return torch.logsumexp(stacked, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a plot of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = torch.meshgrid(2*[torch.linspace(-6, +6, 101)])\n",
    "\n",
    "marg = torch.stack(mesh, dim=-1).flatten(0, -2)\n",
    "\n",
    "z = torch.exp(log_density(marg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\theta_1$\", ylabel=r\"$\\theta_2$\",\n",
    "                    title=\"2x'blob' density\")\n",
    "\n",
    "ax.contourf(*mesh, z.reshape_as(mesh[0]), levels=51, cmap=plt.cm.terrain)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gradient field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = marg.clone().requires_grad_(True)\n",
    "log_density(theta).mean().backward()\n",
    "\n",
    "dz = theta.grad.reshape(*mesh[0].shape, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\theta_1$\", ylabel=r\"$\\theta_2$\",\n",
    "                    title=\"2x'blob' density\")\n",
    "\n",
    "ax.contourf(*mesh, z.reshape_as(mesh[0]), levels=51, cmap=plt.cm.terrain)\n",
    "\n",
    "if True:\n",
    "    ax.quiver(*mesh, dz[..., 0], dz[..., 1], pivot='mid',\n",
    "              color=\"fuchsia\", scale=.5, alpha=0.5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some samplers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proposal:\n",
    "    def sample(self, x=None, n_samples=1):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def log_prob(self, x, at=None):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a measurable space $(\\Omega, \\mathcal{F}, \\mu)$.\n",
    "The Markov Chain sampler needs:\n",
    "* *(proposal)* the transition `kernel`\n",
    "$P \\colon \\Omega \\times \\mathcal{F} \\to [0, 1]$\n",
    "\n",
    "In MC-MC we typically consider kernels over Lebsegue carrier measures\n",
    "$\\mu = d{x}$ and defined by\n",
    "\n",
    "$$\n",
    "P(x, d\\omega)\n",
    "    = q(\\omega \\vert x) \\mu(d\\omega) + r(x) \\delta_x(d\\omega)\n",
    "    \\,, $$\n",
    "\n",
    "where $Q(\\bullet \\vert x) = q(\\cdot \\vert x) \\mu(d\\omega)$ is a nonnegative measure\n",
    "with $Q(\\Omega\\vert x) \\leq 1$ and  $q(x \\vert x) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this notation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, $\\delta_x(\\cdot)$ defines a probability measure on the\n",
    "measurable space $(\\Omega, \\mathcal{F})$ according to\n",
    "\n",
    "$$\n",
    "\\delta_x\n",
    "\\colon \\mathcal{F} \\to [0, 1]\n",
    "\\colon A \\mapsto 1_A(x)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notation is a shorthand for (like in SDE)\n",
    "$$\n",
    "P(x, dy) = q(y \\vert x) dy + r(x) \\delta_x(dy)\n",
    "    \\Leftrightarrow\n",
    "    P(x, A)\n",
    "        = \\int_A q(y \\vert x) dy + r(x) \\delta_x(dy)\n",
    "        = Q(A \\vert x) + r(x) 1_A(x)\n",
    "    $$\n",
    "\n",
    "This implies that for $P(x, \\cdot)$ to be a probability measure\n",
    "we need $r(x) = 1 - Q(\\Omega \\vert x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did i really ned this proof? NO!\n",
    "$\\delta_x(\\emptyset) = 1_\\emptyset(x) = 0$, and\n",
    "$$\n",
    "\\delta_x\\bigl(\\biguplus_{n\\geq 1} A_n\\bigr)\n",
    "    = 1_{\\uplus_{n\\geq 1} A_n}(x)\n",
    "    = \\begin{cases}\n",
    "    1 & \\exists{n\\geq1}\\, x \\in A_n\\\\\n",
    "    0 & \n",
    "    \\end{cases}\n",
    "    = \\sum_{n\\geq 1} 1_{A_n}(x)\n",
    "    = \\sum_{n\\geq 1} \\delta_x(A_n)\n",
    "    \\,. $$\n",
    "\n",
    "Via the MCT this implies that $\\int f \\delta_x(d\\omega) = f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target(Proposal):\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "\n",
    "    def log_prob(self, x, at=None):\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalkProposal(Proposal):\n",
    "    def __init__(self, distribution):\n",
    "        self.distribution = distribution\n",
    "    \n",
    "    def log_prob(self, x, at):\n",
    "        return self.distribution.log_prob(x - at)\n",
    "\n",
    "    def sample(self, x, n_samples=1):\n",
    "        *head, n_features = x.shape\n",
    "\n",
    "        step = self.distribution.sample((*head, n_samples))\n",
    "        return (x.unsqueeze(-2) + step).squeeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetropolisHastingsProposal(Proposal):\n",
    "    def __init__(self, proposal, target):\n",
    "        self.target, self.proposal = target, proposal\n",
    "\n",
    "    def sample(self, x, n_samples=1):\n",
    "        *head, n_features = x.shape\n",
    "        head = [1] if not head else head\n",
    "        \n",
    "        p, q = self.target, self.proposal\n",
    "\n",
    "        x_next = q.sample(x, 1)\n",
    "\n",
    "        log_alpha = p.log_prob(x_next) + q.log_prob(x_next, x)\n",
    "        log_alpha -= p.log_prob(x) + q.log_prob(x, x_next)\n",
    "        alpha = torch.exp(torch.clamp(log_alpha, max=0))\n",
    "\n",
    "        accept = torch.rand_like(log_alpha) < alpha\n",
    "\n",
    "        return torch.where(accept.unsqueeze(-1), x_next, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a random walk transition kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.distributions import MultivariateNormal\n",
    "\n",
    "gauss = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "rwp = RandomWalkProposal(gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhp = MetropolisHastingsProposal(rwp, Target(log_density))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A path plotter using quiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path(path, ax=None, **kwargs):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "\n",
    "    uv, xy = path[1:] - path[:-1], path[:-1]\n",
    "    return ax.quiver(xy[:, 0], xy[:, 1], uv[:, 0], uv[:, 1],\n",
    "                     scale_units='xy', angles='xy', scale=1.,\n",
    "                     **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [torch.randn(5, 2) * 5]\n",
    "for _ in range(500):\n",
    "    paths.append(mhp.sample(paths[-1]))\n",
    "\n",
    "paths = torch.stack(paths, dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the Markov Chain paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = paths.flatten(0, -2).min(0)[0]\n",
    "ur = paths.flatten(0, -2).max(0)[0]\n",
    "\n",
    "mesh = torch.meshgrid(*map(torch.linspace, ll, ur, [201, 201]))\n",
    "marg = torch.stack(mesh, dim=-1).flatten(0, -2)\n",
    "\n",
    "z = torch.exp(log_density(marg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\theta_1$\", ylabel=r\"$\\theta_2$\")\n",
    "\n",
    "ax.contourf(*mesh, z.reshape_as(mesh[0]), levels=21,\n",
    "           cmap=plt.cm.terrain, alpha=0.05, zorder=10)\n",
    "\n",
    "colours = plt.cm.Accent(np.linspace(0, 1, num=len(paths)))\n",
    "for i, col in enumerate(colours):\n",
    "    pts = paths[i].numpy()\n",
    "    plot_path(pts, color=col, alpha=.5)\n",
    "#     ax.scatter(pts[:, 0], pts[:, 1], color=col, s=10, alpha=0.05)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\in \\Omega^{b_1 \\times \\ldots \\times b_p}$ for $\\Omega \\subseteq \\mathbb{R}$\n",
    "and any transition kernel must "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MH kernel is given by the density:\n",
    "$$\n",
    "P(x, dy)\n",
    "    = p(x, y) dy + r(x) \\delta_x(dy)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leap_frog(nabla, q, p, grad=None, eps=0.01):\n",
    "    r\"\"\"Leap-frog integrator for \\dot{p} = - \\nabla_q V(q), \\dot{q} = p\"\"\"\n",
    "    # p_{\\tfrac12} = p_0 - \\tfrac\\epsilon2 \\nabla_q V(q_0)\n",
    "    grad = nabla(q) if grad is None else grad  # .detach()\n",
    "    p.sub_(- eps * grad / 2)\n",
    "\n",
    "    # q_1 = q_0 + \\epsilon p_{\\tfrac12}\n",
    "    q.add_(eps * p)\n",
    "\n",
    "    # p_1 = p_{\\tfrac12} - \\tfrac\\epsilon2 \\nabla_q V(q_1)\n",
    "    grad = nabla(q)\n",
    "    p.sub_(- eps * grad / 2)\n",
    "    return p, q, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor([\n",
    "    [-3., -3.],\n",
    "    [+3., -3.],\n",
    "    [+3., +3.],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.tensor[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triangular_solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(value, mu, loc):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Normal(mu, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.rsample((10,)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.log_prob??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = mu[0].clone().requires_grad_(True)\n",
    "target.log_prob(q).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
