{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a study of Markov Chain MOnte Carlo methods,\n",
    "and is inspired, in part, by these great papers: \n",
    "* A basic overview of sampling methods such as AR, MH and Gibbs\n",
    "  [Chib, Greenberg (1995)](http://web1.sph.emory.edu/users/hwu30/teaching/statcomp/papers/chibGreenbergMH.pdf)\n",
    ". Includes a hybrid MH-AR method (whrere the proposal is sampled from using AR)\n",
    "* A self contained introduction and study of Monte Carlo based on Hamiltonian dynamics\n",
    "  [Betancourt (2017)](https://arxiv.org/abs/1701.02434.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get inflated `rect` for better $2$-d plot aesthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rect(data, dim=0, r=5e-2, a=1e-3):\n",
    "    # get the enclosing rectangle ...\n",
    "    (uu, _), (ll, _) = data.max(dim), data.min(dim)\n",
    "\n",
    "    # ... center it, infalte, ...\n",
    "    cc = (uu + ll) / 2\n",
    "    uu, ll = uu - cc, ll - cc\n",
    "    uu = uu + abs(uu) * r + a\n",
    "    ll = ll - abs(ll) * r - a\n",
    "\n",
    "    # ... and translate back\n",
    "    return ll + cc, uu + cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a multi-modal density: we can evaluate it, know its structure,\n",
    "but not the normalizing constant.\n",
    "\n",
    "$$\n",
    "p(\\theta)\n",
    "    = \\tfrac1Z \\mathop{\\mathrm{exp}} \\bigl(\n",
    "        \\log f(\\theta)\n",
    "    \\bigr)\n",
    "    \\,.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a banana distribution:\n",
    "\n",
    "$$\n",
    "p(x)\n",
    "    \\propto p_{\\mathcal{N}(0, 1)} \\circ \\phi(x)\n",
    "    \\,, $$\n",
    "\n",
    "where $\\phi$ is the based on the\n",
    "[Banana](https://en.wikipedia.org/wiki/Rosenbrock_function)\n",
    "function and given by $\n",
    "\\phi\n",
    "\\colon \\mathbb{R}^2 \\to \\mathbb{R}^2\n",
    "\\colon (x, y) \\mapsto (a x, b (y-x^2))\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_banana_base(x, a=0.75, b=1.05):\n",
    "#     phi = torch.stack([a - x[..., 0], b * (x[..., 1] - x[..., 0]**2)], dim=-1)  # a=1.75, b=5\n",
    "    phi = torch.stack([a * (x[..., 0] - x[..., 1]**2), b * (x[..., 1] - x[..., 0]**2)], dim=-1)\n",
    "    return -0.5 * torch.norm(phi, p=2, keepdim=False, dim=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_density(x):\n",
    "    mu = map(torch.tensor, [(2., 2.), (-2., -2.)])\n",
    "    a, b, s = [+0.75, -0.75], [+3.05, -1.05], [+1, -1]\n",
    "\n",
    "    compo = map(lambda m, a, b, s: log_banana_base(s*(x - m), a, b), mu, a, b, s)\n",
    "    stacked = torch.stack([*compo], dim=0)\n",
    "\n",
    "    return torch.logsumexp(stacked, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_density(x):\n",
    "#     return -0.5 * torch.norm(x, p=2, keepdim=False, dim=-1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a plot of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = torch.meshgrid(2*[torch.linspace(-6, +6, 101)])\n",
    "\n",
    "marg = torch.stack(mesh, dim=-1).flatten(0, -2)\n",
    "\n",
    "z = torch.exp(log_density(marg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\theta_1$\", ylabel=r\"$\\theta_2$\",\n",
    "                    title=\"2x'blob' density\")\n",
    "\n",
    "ax.contourf(*mesh, z.reshape_as(mesh[0]), levels=51, cmap=plt.cm.terrain)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gradient field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = marg.clone().requires_grad_(True)\n",
    "log_density(theta).mean().backward()\n",
    "\n",
    "dz = theta.grad.reshape(*mesh[0].shape, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\theta_1$\", ylabel=r\"$\\theta_2$\",\n",
    "                    title=\"2x'blob' density\")\n",
    "\n",
    "ax.contourf(*mesh, z.reshape_as(mesh[0]), levels=51, cmap=plt.cm.terrain)\n",
    "\n",
    "if True:\n",
    "    ax.quiver(*mesh, dz[..., 0], dz[..., 1], pivot='mid',\n",
    "              color=\"fuchsia\", scale=.5, alpha=0.5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some samplers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proposal:\n",
    "    def sample(self, n_samples=1, *, at=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def log_prob(self, x, *, at=None):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a measurable space $(\\Omega, \\mathcal{F}, \\mu)$.\n",
    "The Markov Chain sampler needs:\n",
    "* *(proposal)* the transition `kernel`\n",
    "$P \\colon \\Omega \\times \\mathcal{F} \\to [0, 1]$\n",
    "\n",
    "In MC-MC we typically consider kernels over Lebsegue carrier measures\n",
    "$\\mu = d{x}$ and defined by\n",
    "\n",
    "$$\n",
    "P(x, d\\omega)\n",
    "    = q(\\omega \\vert x) \\mu(d\\omega) + r(x) \\delta_x(d\\omega)\n",
    "    \\,, $$\n",
    "\n",
    "where $Q(\\bullet \\vert x) = q(\\cdot \\vert x) \\mu(d\\omega)$ is a nonnegative measure\n",
    "with $Q(\\Omega\\vert x) \\leq 1$ and  $q(x \\vert x) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this notation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, $\\delta_x(\\cdot)$ defines a probability measure on the\n",
    "measurable space $(\\Omega, \\mathcal{F})$ according to\n",
    "\n",
    "$$\n",
    "\\delta_x\n",
    "\\colon \\mathcal{F} \\to [0, 1]\n",
    "\\colon A \\mapsto 1_A(x)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notation is a shorthand for (like in SDE)\n",
    "$$\n",
    "P(x, dy) = q(y \\vert x) dy + r(x) \\delta_x(dy)\n",
    "    \\Leftrightarrow\n",
    "    P(x, A)\n",
    "        = \\int_A q(y \\vert x) dy + r(x) \\delta_x(dy)\n",
    "        = Q(A \\vert x) + r(x) 1_A(x)\n",
    "    \\,, $$\n",
    "\n",
    "This implies that for $P(x, \\cdot)$ to be a probability measure\n",
    "we need $r(x) = 1 - Q(\\Omega \\vert x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did i really need this proof? NO!\n",
    "\n",
    "$\\delta_x(\\emptyset) = 1_\\emptyset(x) = 0$, and\n",
    "$$\n",
    "\\delta_x\\bigl(\\biguplus_{n\\geq 1} A_n\\bigr)\n",
    "    = 1_{\\uplus_{n\\geq 1} A_n}(x)\n",
    "    = \\begin{cases}\n",
    "    1 & \\exists{n\\geq1}\\, x \\in A_n\\\\\n",
    "    0 & \n",
    "    \\end{cases}\n",
    "    = \\sum_{n\\geq 1} 1_{A_n}(x)\n",
    "    = \\sum_{n\\geq 1} \\delta_x(A_n)\n",
    "    \\,. $$\n",
    "\n",
    "Via the MCT this implies that $\\int f \\delta_x(d\\omega) = f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target(Proposal):\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "\n",
    "    def log_prob(self, x, *, at=None):\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalkProposal(Proposal):\n",
    "    def __init__(self, distribution):\n",
    "        self.distribution = distribution\n",
    "    \n",
    "    def log_prob(self, x, *, at):\n",
    "        return self.distribution.log_prob(x - at)\n",
    "\n",
    "    def sample(self, n_samples=1, *, at):\n",
    "        *head, n_features = at.shape\n",
    "\n",
    "        step = self.distribution.sample((*head, n_samples))\n",
    "        return (at.unsqueeze(-2) + step).squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\in \\Omega^{b_1 \\times \\ldots \\times b_p}$ for $\\Omega \\subseteq \\mathbb{R}$\n",
    "and any transition kernel must "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MH kernel is given by the density:\n",
    "$$\n",
    "P(x, dy)\n",
    "    = p(x, y) dy + r(x) \\delta_x(dy)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropilis-Hastings Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some transition density (or pmf) $p(y \\vert x)$ the transition kernel\n",
    "$$\n",
    "P(x, dy)\n",
    "%     = p(dy \\vert x) + r(x) \\delta_x(dy)\n",
    "    = p(y \\vert x) dy + r(x) \\delta_x(dy)\n",
    "    \\,, $$\n",
    "with $r(x) = 1 - \\int_\\Omega p(dy \\vert x) \\leq 1$ has $\\pi$ as the stationary\n",
    "distribution if $p$ complies with the detailed (micro-) balance condition:\n",
    "$\n",
    "\\pi(dx) p(dy \\vert x) = \\pi(dy) p(dx \\vert y)\n",
    "$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int_\\Omega \\pi(dx) P(x, B)\n",
    "    &= \\int_\\Omega \\pi(dx) \\int_B P(x, dy)\n",
    "    = \\int_\\Omega \\int_B p(dy \\vert x) \\pi(dx)\n",
    "      + \\int_\\Omega \\int_B r(x) \\delta_x(dy) \\pi(dx)\n",
    "    \\\\\n",
    "    &= \\int^x_\\Omega \\int^y_B p(dy \\vert x) \\pi(dx)\n",
    "      + \\int_\\Omega r(x) 1_B(x) \\pi(dx)\n",
    "    = \\int^y_B \\int^x_\\Omega p(dx \\vert y) \\pi(dy) + \\int_B r(x) \\pi(dx)\n",
    "    \\\\\n",
    "    &= \\int_B (1 - r(y)) \\pi(dy) + \\int_B r(x) \\pi(dx)\n",
    "    = \\pi(B)\n",
    "    \\,,\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus for a chosen proposal density $q(dy \\vert x)$ we need to find\n",
    "a transition density $p(dy \\vert x)$, that satisfies the balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(heuristic)** Let's use importance sampling (analogue of): introduce a rv that\n",
    "controls the transitions and adjusts the resulting `density` so that is has the\n",
    "needed mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\n",
    "p(y\\vert x)\n",
    "    = q(y\\vert x) \\alpha(y \\vert x)\n",
    "$, where $\\alpha$ enforces `reversibility`:\n",
    "$$\n",
    "\\pi(dx) q(dy\\vert x) \\alpha(y \\vert x)\n",
    "    = \\pi(dy) q(dx\\vert y) \\alpha(x \\vert y)\n",
    "    \\,. $$\n",
    "\n",
    "Assuming $\\pi(dx) = \\pi(x) dx$ and $q(dy \\vert x) = q(y\\vert x) dy$ we\n",
    "may observe the following:\n",
    "* If $\\pi(x) q(y\\vert x) > \\pi(y) q(x\\vert y)$ then $\n",
    "\\alpha(y \\vert x)\n",
    "    = \\tfrac{\\pi(x) q(y\\vert x)}{\\pi(y) q(x\\vert y)}\n",
    "$ and $\\alpha(x \\vert y) = 1$\n",
    "\n",
    "* If $\\pi(x) q(y\\vert x) < \\pi(y) q(x\\vert y)$ then $\n",
    "\\alpha(x \\vert y)\n",
    "    = \\tfrac{\\pi(y) q(x\\vert y)}{\\pi(x) q(y\\vert x)}\n",
    "$ and $\\alpha(y \\vert x) = 1$\n",
    "\n",
    "Thus the sought $\\alpha$ is\n",
    "$$\n",
    "\\alpha(y\\vert x)\n",
    "    = \\min\\Bigl\\{\n",
    "        1, \\frac{\\pi(x) q(y\\vert x)}{\\pi(y) q(x\\vert y)}\n",
    "    \\Bigr\\}\n",
    "    \\,. $$\n",
    "\n",
    "It is called the *probability of move*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the basic step of MH MC sampler is: given $x_t$ do\n",
    "1. sample $y \\sim q(y \\vert x_t)$\n",
    "2. independently draw $u \\sim \\mathrm{U}[0, 1]$ and\n",
    "    * set $x_{t+1} = y$ if $u \\leq \\alpha(y\\vert x_t)$\n",
    "    * put $x_{t+1} = x_t$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetropolisHastingsProposal(Proposal):\n",
    "    def __init__(self, proposal, target):\n",
    "        self.target, self.proposal = target, proposal\n",
    "\n",
    "    def sample(self, n_samples=1, *, at):\n",
    "        *head, n_features = at.shape\n",
    "        head = [1] if not head else head\n",
    "\n",
    "        p, q = self.target, self.proposal\n",
    "\n",
    "        assert n_samples == 1\n",
    "        prop, curr = q.sample(n_samples, at=at), at\n",
    "        # curr = curr.unsqueeze(-2)  # DON'T : dimension growth! but gives branching paths!\n",
    "\n",
    "        # \\log \\pi(prop) q(curr \\vert prop)\n",
    "        log_alpha  = p.log_prob(prop) + q.log_prob(curr, at=prop)\n",
    "\n",
    "        # - \\log \\pi(curr) q(prop \\vert curr)\n",
    "        log_alpha -= p.log_prob(curr) + q.log_prob(prop, at=curr)\n",
    "\n",
    "        alpha = torch.exp(torch.clamp(log_alpha, max=0))\n",
    "        accept = torch.rand_like(log_alpha) < alpha\n",
    "        # print(accept.float().mean(), alpha)\n",
    "        self.alpha_ = alpha\n",
    "\n",
    "        return torch.where(accept.unsqueeze(-1), prop, curr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Betancourt (2017) p.11](https://arxiv.org/abs/1701.02434)\n",
    "* **(first phase)** MC travels towards the typical set. MC-based\n",
    "estimators have strong bias (*burn-in*)\n",
    "\n",
    "* **(second phase)** MC `persists through the first sojourn across the typical set`.\n",
    "Accuracy of estimators improves as the bias from burn-in dampens\n",
    "\n",
    "* **(third phase)** MC continues and gradually refines its exploration\n",
    "of the typical set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a random walk transition kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "gauss = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "rwp = RandomWalkProposal(gauss)\n",
    "\n",
    "mhp = MetropolisHastingsProposal(rwp, Target(log_density))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A path plotter using quiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path(path, ax=None, **kwargs):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "\n",
    "    uv, xy = path[1:] - path[:-1], path[:-1]\n",
    "\n",
    "    size = np.linalg.norm(uv, axis=-1, keepdims=False)\n",
    "    stuck = path[1:][size == 0]\n",
    "\n",
    "    ax.scatter(stuck[:, 0], stuck[:, 1], c=\"k\", alpha=0.1)\n",
    "\n",
    "    return ax.quiver(xy[:, 0], xy[:, 1], uv[:, 0], uv[:, 1],\n",
    "                     scale_units='xy', angles='xy', scale=1.,\n",
    "                     **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "paths = [torch.randn(15, 2) * 15]\n",
    "# paths = [torch.randn(1, 2).repeat(15, 1) * 15]\n",
    "for _ in range(500):\n",
    "    paths.append(mhp.sample(1, at=paths[-1]))\n",
    "\n",
    "paths = torch.stack(paths, dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the Markov Chain paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll, ur = get_rect(paths.flatten(0, -2))\n",
    "\n",
    "mesh = torch.meshgrid(*map(torch.linspace, ll, ur, [201, 201]))\n",
    "marg = torch.stack(mesh, dim=-1).flatten(0, -2)\n",
    "\n",
    "z = torch.exp(log_density(marg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "fig.patch.set_alpha(1.0)\n",
    "\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\theta_1$\", ylabel=r\"$\\theta_2$\")\n",
    "\n",
    "ax.contourf(*mesh, z.reshape_as(mesh[0]), levels=21,\n",
    "           cmap=plt.cm.terrain, alpha=0.05, zorder=10)\n",
    "\n",
    "colours = plt.cm.Accent(np.linspace(0, 1, num=len(paths)))\n",
    "for i, col in enumerate(colours):\n",
    "    pts = paths[i].numpy()\n",
    "    plot_path(pts, color=col, alpha=.5)\n",
    "#     ax.scatter(pts[:, 0], pts[:, 1], color=col, s=10, alpha=0.05)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Betancourt (2017) pp. 16-19](https://arxiv.org/abs/1701.02434)\n",
    "\n",
    "Although MH with Random Walk proposal (aka RandomWalk Metropolis, RWMH) is simple\n",
    "and intuitive clear, it drammatically suffers from the curse of dimensionality\n",
    "and the complexity of the target distribution.\n",
    "\n",
    "> ... the volume exterior to the typical set overwhelms the interior volume\n",
    "and almost every RWMH chain gets stuck outside of the typical set towards the\n",
    "tails, due to low acceptance rate, induced by negligible densities. ... In the\n",
    "worst case RWMH won't even complete a single sojourn through the typical set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Hamiltonian MC is to use first order information,\n",
    "$\\nabla_\\theta \\log \\pi(\\theta)$, about the target distribution $\\pi(\\theta)$\n",
    "to make informed moves towards the typical set. However, by itself\n",
    "the gradient pulls towrads a mode of $\\pi(\\theta)$ and would make\n",
    "the chain collapse in it, which is not the typical set (what is it then?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in HMC we consider $\\pi(\\theta, m) = \\pi(\\theta) p(m \\vert \\theta)$\n",
    "where we have introduced auxiliary random varaible, momentum $m$. This\n",
    "`lifts the target distribution onto a joint probability distribution on\n",
    "pahse space` If the momentum is marginalized, then the original target\n",
    "density is recovred, which means that during sampling we can simply discard\n",
    "$m$ when requesting a sampel of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\\pi(\\theta, m) = \\exp\\bigl\\{ - H(\\theta, m) \\bigr\\}$. Then for a\n",
    "trajectory $t\\mapsto (\\theta_t, m_t)$ so stay on the level set of $H$\n",
    "we must have $\\tfrac{d}{d t} H(\\theta_t, m_t) = 0$, i.e.\n",
    "\n",
    "$$\n",
    "dH = \\nabla_\\theta^\\top H(\\theta_t, m_t) \\dot{\\theta}_t dt\n",
    "    + \\nabla_m^\\top H(\\theta_t, m_t) \\dot{m}_t dt\n",
    "    = 0\n",
    "    \\,, $$\n",
    "\n",
    "which is satisfied when\n",
    "$$\n",
    "\\dot{\\theta}_t = \\nabla_m H(\\theta_t, m_t)\n",
    "    \\,,\\,\n",
    "    \\dot{m}_t = \\nabla_\\theta H(\\theta_t, m_t)\n",
    "    \\,. $$\n",
    "\n",
    "Typically the Hamiltonian is decomposed as\n",
    "\n",
    "$$\n",
    "H(\\theta, m)\n",
    "    = - \\log \\pi(\\theta) p(m \\vert \\theta)\n",
    "    = \\underbrace{- \\log \\pi(\\theta)}_{\\text{potential}}\n",
    "    + \\bigl(\n",
    "        \\underbrace{- \\log p(m \\vert \\theta)}_{\\text{kinetic}}\n",
    "    \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Betancourt (2017) pp. 27](https://arxiv.org/abs/1701.02434)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the kinetic energy term is what determines the interaction if the Chain with the target.\n",
    "Herein lies the scope of HMC design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomness in HMC scheme comes from the randomness of the `momentun lift`,\n",
    "whereas the traversal of the level of $H$ is deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the HMC trajectory journies along the level set in the phase-space,\n",
    "it is natural to factorize the canonical distribution $\\pi(\\theta, m)$,\n",
    "byt foliating into concentric level sets:\n",
    "\n",
    "$$\n",
    "\\pi(\\theta, m)\n",
    "    = \\pi(x_E \\vert H(x_E) = E)\n",
    "    \\, \\pi(H(x_E) = E)\n",
    "    \\,. $$\n",
    "\n",
    "Lifts determine jumps beteween the energy levels, while each tajectory explores\n",
    "the corresponding level set $\\{H(\\theta, m) = E\\}$. Thus we get two phases\n",
    "\n",
    "* deterministic traversal of energy level sets (how long we integrate)\n",
    "* stochastic exploration between level sets (how quickly jumps `diffuse accross energies typical to the energy marginal distribution`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following kinetic energy is called Euclidean-Gaussian if $G(\\theta) = \\Sigma$,\n",
    "and Riemannian-Gaussian if $G(\\theta) = $.\n",
    "\n",
    "* we can optimize $\\Sigma$ using the extendend burn-in pahse\n",
    "[Betancourt (2017) pp. 31](https://arxiv.org/abs/1701.02434)\n",
    "\n",
    "* if $G(\\theta)$ resembles the Hessian of the target, i.e. its Fisher info-matrix,\n",
    "then the energy level exploration would be uniform and efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the integration time: if we integrate for a short period of time,\n",
    "we risk having insufficient diversity on the samples as they would tend to\n",
    "clump together. On the other hand, long integration times can degrade exploration\n",
    "in case when the level sets are `topologically` compact (is there any other\n",
    "notion of compactness?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Ergodicity for orbits $\\phi = \\{(\\theta_t, m_t)\\colon t \\geq 0\\}$ states\n",
    "that a uniform temporal sample form a trajectory resemples a uniform spatial sample\n",
    "from $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplectic integrators [Betancourt (2017) pp. 36](https://arxiv.org/abs/1701.02434)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to integrate something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cauchy problem for ODE $$\n",
    "\\dot{x}\n",
    "    = A x\\,, x(0) = x_0\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler(x, f, eps=1e-3):\n",
    "    return x + f(x) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = lambda x: np.dot(x, np.array([\n",
    "    [-1., +2.],\n",
    "    [-10., -1.],\n",
    "]))\n",
    "\n",
    "grad = lambda x: np.dot(x, np.array([\n",
    "    [0, +1.],\n",
    "    [-1., 0],\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [np.random.randn(5, 2)]\n",
    "for _ in range(100):\n",
    "    paths.append(euler(paths[-1], grad, eps=1e-1))\n",
    "\n",
    "paths = np.stack(paths, axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "colours = plt.cm.Reds(np.linspace(0.5, 1, num=len(paths)))\n",
    "for i, col in enumerate(colours):\n",
    "    plot_path(paths[i], color=col, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_mesh = np.meshgrid(\n",
    "    np.linspace(-5, +5, num=101),\n",
    "    np.linspace(-5, +5, num=101),\n",
    ")\n",
    "np_marg = np.stack(np_mesh, axis=-1).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv = grad(np_marg).reshape(*np_mesh[0].shape, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, xlabel=r\"$\\theta_1$\", ylabel=r\"$\\theta_2$\",\n",
    "                    title=\"2x'blob' density\")\n",
    "\n",
    "ax.quiver(*np_mesh, uv[..., 0], uv[..., 1], pivot='mid',\n",
    "          color=\"fuchsia\", scale=2000., alpha=0.5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Hamiltonian MonteCarlo we need a hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets introduce an auxiliary variable $q \\approx \\dot{\\theta}$ with\n",
    "$p(q\\vert \\theta) = \\mathcal{N}(q \\vert 0, G(\\theta))$. Then the\n",
    "joint density of $(\\theta, q)$\n",
    "\n",
    "$$\n",
    "p(\\theta, q)\n",
    "    = p(\\theta) p(q\\vert \\theta)\n",
    "    \\propto \\exp{(- H(\\theta, q))}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(\\theta, q)\n",
    "    = - \\ell(\\theta)\n",
    "    + \\tfrac12\\log\\det 2 \\pi G(\\theta)\n",
    "    + \\tfrac12 q^\\top G(\\theta)^{-1} q\n",
    "    \\,, $$\n",
    "with $\\ell(\\theta) = \\log p(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamiltonian dynamics in the case of $q$ behaving like a momentum\n",
    "of $\\theta$ is\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\dot{\\theta}\n",
    "        =& \\partial_q H(\\theta, q)\n",
    "        \\,, \\\\\n",
    "   - \\dot{q}\n",
    "        =& \\partial_\\theta H(\\theta, q)\n",
    "        \\,,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the derivative of $\\log-\\det$ is\n",
    "\n",
    "$$\n",
    "\\partial_\\theta \\log \\det G(\\theta)\n",
    "    = - \\partial_\\theta \\log \\det G(\\theta)^{-1}\n",
    "    = - \\mathrm{tr}\n",
    "        \\tfrac1{\\det G(\\theta)^{-1}} (\\det G(\\theta)^{-1})\n",
    "            G(\\theta)^\\top \\partial_\\theta G(\\theta)^{-1}\n",
    "    = - \\mathrm{tr} G(\\theta) \\partial_\\theta G(\\theta)^{-1}\n",
    "    \\,, $$\n",
    "\n",
    "and the second term is\n",
    "\n",
    "$$\n",
    "\\partial_\\theta \\tfrac12 q^\\top G(\\theta)^{-1} q\n",
    "    = \\tfrac12 \\mathrm{tr} q q^\\top \\partial_\\theta G(\\theta)^{-1}\n",
    "    \\,. $$\n",
    "\n",
    "And $\n",
    "\\partial_\\theta G(\\theta)^{-1}\n",
    "    = - G(\\theta)^{-1}\n",
    "        \\bigl( \\partial_\\theta G(\\theta) \\bigr)\n",
    "    G(\\theta)^{-1}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics is thus\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\dot{\\theta}\n",
    "        =& G(\\theta)^{-1} q\n",
    "        \\,, \\\\\n",
    "    - \\dot{q}\n",
    "%         =& - \\partial_\\theta \\ell(\\theta)\n",
    "%            + \\tfrac12 \\mathrm{tr} \\bigl(\n",
    "%                q q^\\top - G(\\theta)\n",
    "%            \\bigr) \\partial_\\theta G(\\theta)^{-1}\n",
    "        =& - \\partial_\\theta \\ell(\\theta)\n",
    "           - \\tfrac12 \\mathrm{tr} G(\\theta)^{-1} \\bigl(\n",
    "               q q^\\top - G(\\theta)\n",
    "           \\bigr) G(\\theta)^{-1}\n",
    "           \\partial_\\theta G(\\theta)\n",
    "        \\,,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $G(\\theta) = I$ we have\n",
    "\n",
    "$$\n",
    "\\dot{\\theta} = q\n",
    "    \\,, \\dot{q} = \\partial_\\theta \\ell(\\theta)\n",
    "    \\,, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ddot{x} = F(x)\n",
    "    \\Leftrightarrow\n",
    "    \\dot{x} = v\n",
    "    \\,, \\dot{v} = F(x)\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix}\n",
    "    \\dot{q} \\\\ \\dot{p}\n",
    "\\end{pmatrix}\n",
    "    = \\begin{pmatrix}\n",
    "        0 & \\partial_p \\\\\n",
    "        - \\partial_q & 0\n",
    "    \\end{pmatrix}\n",
    "    H(q, p)\n",
    "    \\,, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate gradients of a scalar function in bulk, observe the following:\n",
    "$$\n",
    "(\\nabla_\\theta f(\\theta_i))_{i}\n",
    "    \\colon (\\theta_i)_{i} \\mapsto\n",
    "        \\bigl(\n",
    "            \\tfrac{\\partial}{\\partial \\theta_j}\n",
    "                \\sum_i f(\\theta_i)\n",
    "        \\bigr)_{ji} \\mathbf{1}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad(x, f):\n",
    "    theta = x.clone().requires_grad_(True)\n",
    "    f(theta).sum().backward()\n",
    "    return theta.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def hamiltonian(x, q):\n",
    "    return - log_density(x) + 0.5 * torch.norm(q, p=2, dim=-1)**2\n",
    "\n",
    "x = torch.randn(10, 2).requires_grad_(True)\n",
    "q = torch.randn_like(x).requires_grad_(True)\n",
    "\n",
    "par = x, q\n",
    "f = hamiltonian\n",
    "\n",
    "dx, dq = grad(f(*par).sum(), par, create_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(15, 2)\n",
    "q = torch.randn_like(x)\n",
    "eps = 5e-2\n",
    "\n",
    "# applies the one-step leapforg integrator for \\ddot{x} = \\nabla_x \\ell(x)\n",
    "r\"\"\"Leap-frog integrator for \\dot{x} = q, \\dot{q} = \\nabla_x \\ell(x)\"\"\"\n",
    "\n",
    "paths = [x]\n",
    "grad = batch_grad(x, log_density)\n",
    "for _ in range(2500):\n",
    "    # (todo) add stopped process t \\wedge n_j\n",
    "    # (todo) understand what NUTS does and it does\n",
    "    # (todo) investigate rare NANs\n",
    "\n",
    "    # leap-frog integrator of \\ddot{x} = f(x)\n",
    "    # p_{\\tfrac12} = p_0 + \\nabla_q V(q_0) \\tfrac\\epsilon2\n",
    "    q_half = q + grad * eps / 2\n",
    "\n",
    "    # q_1 = q_0 + p_{\\tfrac12} \\epsilon\n",
    "    x = x + q_half * eps\n",
    "\n",
    "    # p_1 = p_{\\tfrac12} + \\nabla_q V(q_1) \\tfrac\\epsilon2\n",
    "    grad = batch_grad(x, log_density)\n",
    "    q = q_half + grad * eps / 2\n",
    "\n",
    "    paths.append(x)\n",
    "\n",
    "paths = torch.stack(paths, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll, ur = get_rect(paths.flatten(0, -2))\n",
    "\n",
    "mesh = torch.meshgrid(*map(torch.linspace, ll, ur, [201, 201]))\n",
    "marg = torch.stack(mesh, dim=-1).flatten(0, -2)\n",
    "\n",
    "z = torch.exp(log_density(marg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.contourf(*mesh, z.reshape_as(mesh[0]), levels=51, cmap=plt.cm.terrain)\n",
    "\n",
    "colours = plt.cm.Reds(np.linspace(0.5, 1, num=len(paths)))\n",
    "for i, col in enumerate(colours):\n",
    "    plot_path(paths[i][-100:], color=col, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leap_frog(nabla, q, p, grad=None, eps=0.01):\n",
    "    r\"\"\"Leap-frog integrator for \\dot{p} = - \\nabla_q V(q), \\dot{q} = p\"\"\"\n",
    "    # p_{\\tfrac12} = p_0 - \\tfrac\\epsilon2 \\nabla_q V(q_0)\n",
    "    grad = nabla(q) if grad is None else grad  # .detach()\n",
    "    p.sub_(- eps * grad / 2)\n",
    "\n",
    "    # q_1 = q_0 + \\epsilon p_{\\tfrac12}\n",
    "    q.add_(eps * p)\n",
    "\n",
    "    # p_1 = p_{\\tfrac12} - \\tfrac\\epsilon2 \\nabla_q V(q_1)\n",
    "    grad = nabla(q)\n",
    "    p.sub_(- eps * grad / 2)\n",
    "    return p, q, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor([\n",
    "    [-3., -3.],\n",
    "    [+3., -3.],\n",
    "    [+3., +3.],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.tensor[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triangular_solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(value, mu, loc):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Normal(mu, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.rsample((10,)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.log_prob??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = mu[0].clone().requires_grad_(True)\n",
    "target.log_prob(q).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
