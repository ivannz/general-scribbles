{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a measurable space $(\\Omega, \\mathcal{F})$. A map\n",
    "$Q\\colon\\Omega \\times \\mathcal{F} \\to [0, +\\infty)$ is a transition\n",
    "kernel if\n",
    "\n",
    "* $Q(\\cdot, A)$ is measurable map for any $A \\in \\mathcal{F}$\n",
    "\n",
    "* $Q(\\omega, \\cdot)$ is a finite measure on $(\\Omega, \\mathcal{F})$ \n",
    "  for any $\\omega \\in \\Omega$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any measurable map $f$ we can define a measurable map\n",
    "\n",
    "$$\n",
    "    T f\n",
    "    \\colon x \\mapsto \\int f(\\omega) \\, Q(x, d\\omega)\n",
    "    \\,. $$\n",
    "\n",
    "\n",
    "For a measure $\\lambda$ on $(\\Omega, \\mathcal{F})$ we can similarly\n",
    "define a measure\n",
    "\n",
    "$$\n",
    "    T^* \\lambda\n",
    "    \\colon A \\mapsto \\int Q(\\omega, A) \\lambda(d\\omega)\n",
    "    \\,. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dual pairing:\n",
    "\n",
    "$$\n",
    "\\langle\n",
    "    f, \\lambda\n",
    "\\rangle = \\int f(\\omega) \\lambda(d\\omega)\n",
    "    \\,. $$\n",
    "\n",
    "We can show that $\n",
    "\\langle\n",
    "    T f, \\lambda\n",
    "\\rangle = \\langle\n",
    "        f, T^* \\lambda\n",
    "    \\rangle\n",
    "$ via Fubini (?) theorem:\n",
    "\n",
    "$$\n",
    "\\int T f(x) \\lambda(dx)\n",
    "    = \\int \\int f(\\omega) \\, Q(x, d\\omega) \\lambda(dx)\n",
    "    % = \\iint f(\\omega) \\lambda(dx) Q(x, d\\omega)\n",
    "    = \\int f(\\omega) \\int \\lambda(dx) Q(x, d\\omega)\n",
    "    = \\int f(\\omega) (T^* \\lambda)(d\\omega)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A display device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def display(env, fps=15):\n",
    "    if fps > 0:\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        sleep(1. / fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment simulation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpd.tools.delayed import DelayedKeyboardInterrupt\n",
    "\n",
    "def run(env, policy, fps=15):\n",
    "    state, terminate = env.reset(), False\n",
    "\n",
    "    display(env, fps)\n",
    "    with DelayedKeyboardInterrupt(\"ignore\") as stop:\n",
    "        while not (terminate or stop):\n",
    "    \n",
    "            # take an action and get a response form the environment\n",
    "            action = policy(env, state)\n",
    "            state_prime, reward, terminate, info = env.step(action)\n",
    "\n",
    "            # return the result\n",
    "            yield state, action, reward, state_prime, terminate\n",
    "            state = state_prime\n",
    "\n",
    "            # render the enviroment\n",
    "            display(env, fps)\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text import FrozenLakeEnv, CliffWalkingEnv\n",
    "\n",
    "env = FrozenLakeEnv(map_name=\"8x8\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env):\n",
    "    return {\n",
    "        state: [(1. / env.nA, action) for action in kernel]\n",
    "        for state, kernel in env.P.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFF\u001b[41mH\u001b[0mFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "def random(env, state=None):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "episode = [*run(env, random, fps=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Whiteson lecture on MLSS 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP -- classical formal model of a sequential decision problem:\n",
    "\n",
    "* fully-observable, stationary, and possibly stochastic environment\n",
    "\n",
    "* discrete states $S$ and actions $A_s$ for each $s \\in S$\n",
    "\n",
    "* transition kernel $s\\to z \\colon z\\sim q(z\\mid s, a)$ on $S$\n",
    "\n",
    "* reward distributuion $q(r\\mid s, a, s')$ when transitioning $s \\to s'$ under $a$\n",
    "\n",
    "* aplanning horizon or a discount factor $\\gamma \\in (0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov property\n",
    "$p(z_{t+1}, r_{t+1}\\mid s_t, a_t) = p(z_{t+1}, r_{t+1}\\mid s_t, a_t, s_{:t}, a_{:t})$\n",
    "\n",
    "* Reactive policies $a\\sim \\pi(a\\mid s)$\n",
    "* deterministic policies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State-value** of a policy $\n",
    "v^\\pi(s) = \\mathbb{E}_\\pi \\bigl(\n",
    "    \\sum_{k\\geq 1} \\gamma r_{t+k+1}\n",
    "    \\big\\vert s_t = s\n",
    "\\bigr)\n",
    "$ and **action-value** $\n",
    "Q^\\pi(s, a) = \\mathbb{E}_\\pi \\bigl(\n",
    "    \\sum_{k\\geq 1} \\gamma r_{t+k+1}\n",
    "    \\big\\vert s_t = s, a_t = a\n",
    "\\bigr)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman fixed-point equation for $v^\\pi$:\n",
    "$$\n",
    "v^\\pi(s)\n",
    "    = \\mathbb{E}_{a\\sim \\pi(s)} \\mathbb{E}_{s' \\sim q(s'\\mid s, a)}\n",
    "        r(s, a, s') + \\gamma v^\\pi(s')\n",
    "    \\,, $$\n",
    "\n",
    "and for $q^\\pi$\n",
    "\n",
    "$$\n",
    "q^\\pi(s, a)\n",
    "    = \\mathbb{E}_{s' \\sim q(s'\\mid s, a)}\n",
    "        r(s, a, s') + \\gamma \\mathbb{E}_{a'\\sim \\pi(s')} q^\\pi(s', a')\n",
    "    = \\mathbb{E}_{s' \\sim q(s'\\mid s, a)}\n",
    "        r(s, a, s') + \\gamma v^\\pi(s')\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policies can be partially oredered by their value function. And\n",
    "all optimal policies share the same optimal state valeu function\n",
    "$v^*(\\cdot) = \\max_\\pi v^\\pi(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellamn optimality conditions for $v^*$ and are\n",
    "\n",
    "$$\n",
    "v^*(s)\n",
    "    = \\max_{a\\in A_s} \\mathbb{E}_{s' \\sim q(s'\\mid s, a)}\n",
    "        r(s, a, s') + \\gamma v^*(s')\n",
    "    \\,, $$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "q^*(s, a)\n",
    "    = \\mathbb{E}_{z \\sim q(s'\\mid s, a)}\n",
    "        r(s, a, s') + \\gamma \\max_{a'\\in A_{s'}}  q^*(s', a')\n",
    "    \\,, $$\n",
    "\n",
    "respectively. The optimal policy is greedy with respect to $q$:\n",
    "\n",
    "$$\n",
    "\\pi^*(s)\n",
    "    = \\delta_{a^*_s}\n",
    "    \\,, \\text{ for }\n",
    "    a^*_s = \\arg\\max_{a\\in A_s} q^*(s, a)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman operator for a policy $\\pi\\colon \\mathcal{S} \\to \\Delta_A$\n",
    "$$\n",
    "T_\\pi(v)\n",
    "\\colon s \\mapsto \\mathbb{E}_{a\\sim \\pi(a\\mid s)}\n",
    "    \\mathbb{E}_{z\\sim q(z\\mid s, a)} r(s, a, z) + \\gamma v(z)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_state_reward(states, value, gamma=1.0):\n",
    "    # kernel -- list of next state-rewards with probabilities\n",
    "    return sum(\n",
    "        prob * (reward + gamma * value[state])\n",
    "        for prob, state, reward, term in states\n",
    "    )\n",
    "\n",
    "def expected_action_reward(actions, kernel, value, gamma=1.0):\n",
    "    # policy -- list of actions with probabilities\n",
    "    return sum(\n",
    "        prob * expected_state_reward(kernel[action], value, gamma)\n",
    "        for prob, action in actions\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy evaluation is performed via the fixed point iterations:\n",
    "\n",
    "* repeat $v_{t+1} \\leftarrow T_\\pi(v_t)$ until convergence in $\\|\\cdot\\|_\\infty$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma=1.0, atol=1e-8):\n",
    "    value, delta = {state: 0. for state in env.P}, float(\"+inf\")\n",
    "    while delta > atol:\n",
    "        Tv = {\n",
    "            state: expected_action_reward(policy[state], kernel, value, gamma)\n",
    "            for state, kernel in env.P.items()\n",
    "        }\n",
    "        \n",
    "        delta = max(abs(a - b) for a, b in zip(Tv.values(), value.values()))\n",
    "        value = Tv\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $q$-function of $v$ is\n",
    "$$\n",
    "q_\\infty(s, a)\n",
    "    = \\mathbb{E}_{s'\\sim q(s'\\mid s, a)}\n",
    "        r(s, a, s') + \\gamma v_\\infty(s')\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_fun(env, value, gamma=1.0):\n",
    "    return {\n",
    "        state: {\n",
    "            action: expected_state_reward(states, value, gamma)\n",
    "            for action, states in kernel.items()\n",
    "        }\n",
    "        for state, kernel in env.P.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate a random exploration policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.\n",
    "\n",
    "policy = random_policy(env)\n",
    "value = evaluate_policy(env, policy, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stationary $v$-function induces the following policy (consistent with it):\n",
    "$$\n",
    "\\pi(s)\n",
    "    = \\delta_{a_s}\n",
    "    \\,, \\text{ for } a_s = \\arg \\max_{a \\in A_s} Q(s, a) \n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_from_q(q):\n",
    "    def _policy(env, state):\n",
    "        actions, expected = zip(*q[state].items())\n",
    "        return actions[np.argmax(expected)]\n",
    "\n",
    "    return _policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "greedy = get_greedy_from_q(q_fun(env, value, gamma))\n",
    "episode = [*run(env, greedy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fixed point $v^\\pi$ is the true value function of $\\pi$. The associated $q$ function ca be used to reason about improvements in the policy $\\pi$:\n",
    "if at some $s\\in S$ we have $q^\\pi(s, a_s) > v^\\pi(s)$ for some $a_s \\in A_s$ then the new policy $\\hat{\\pi}(\\cdot) = \\pi(\\cdot)$ but $\\hat{\\pi}(s) = \\delta_{a_s}$ is strictly better than $\\pi$ (w.r.t $v$-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to all states yields the **greedy** policy improvement:\n",
    "\n",
    "$$\n",
    "\\pi_{t+1}(s) \\in \\arg\\max_{a\\in A_s} q^{\\pi_t}(s, a)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_policy(q):\n",
    "    policy = {}\n",
    "    for state, value in q.items():\n",
    "        # put equal mass on the actions with the maximal expected reward\n",
    "        v_max = max(value.values())\n",
    "        action = [a for a, v in value.items() if v >= v_max]\n",
    "        policy[state] = [(1. / len(action), a) for a in action]\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\pi_{t+1} = \\pi_t$ then $v^{\\pi_t} = v^{\\pi_{t+1}} = v$, which\n",
    "satisfies the Bellamn Optimiality principle:\n",
    "\n",
    "* $T(v) = v$ for\n",
    "$$\n",
    "T(v)\n",
    "\\colon S \\to \\mathbb{R}\n",
    "\\colon s \\mapsto \\max_{a\\in A_s}\n",
    "    \\mathbb{E}_{z\\sim q(s'\\mid s, a)} r(s, a, s') + \\gamma v(s')\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=1.0, atol=1e-8):\n",
    "    policy = random_policy(env)\n",
    "    value, delta = evaluate_policy(env, policy, gamma), float(\"+inf\")\n",
    "    while delta > atol:\n",
    "        policy = better_policy(q_fun(env, value, gamma))\n",
    "        new = evaluate_policy(env, policy, gamma)\n",
    "\n",
    "        delta = max(abs(a - b) for a, b in zip(new.values(), value.values()))\n",
    "        value = new\n",
    "\n",
    "    return value, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, policy = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes a stochastic policy and uses MAP action prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_from_pi(policy):\n",
    "    def _policy(env, state):\n",
    "        probs, actions = zip(*policy[state])\n",
    "        return actions[np.argmax(probs)]\n",
    "\n",
    "    return _policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "episode = [*run(env, get_greedy_from_pi(policy))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_from_pi(policy):\n",
    "    def _policy(env, state):\n",
    "        probs, actions = zip(*policy[state])\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    return _policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "episode = [*run(env, get_random_from_pi(policy))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (original) Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MDP:\n",
    "* $q(z\\mid s, a)$ the transition kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (original) Bellman operator is\n",
    "$$\n",
    "T(v)\n",
    "\\colon s \\mapsto \\max_{a\\in A_s} \\mathbb{E}_{z\\sim q(z\\mid s, a)} r(s, a, z) + \\gamma v(z)\n",
    "    \\,. $$\n",
    "\n",
    "It can be shown that $T$ is a contraction mapping w.r.t $\\|\\cdot\\|_\\infty$ for $\\gamma \\in (0, 1)$,\n",
    "and thus the fixed point iteration converges to a $v_*$:\n",
    "* repeat $v_{t+1} \\leftarrow T(v_t)$ until convergence in $\\|\\cdot\\|_\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value function implies an optimal (greedy) poilcy:\n",
    "\n",
    "$$\n",
    "\\pi(s) \\in \\arg\\max_{a\\in A_s}\n",
    "    \\mathbb{E}_{z\\sim q(z\\mid s, a)} r(s, a, z) + \\gamma v^*(z)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_value(kernel, value, policy=None, gamma=1.0):\n",
    "    return max(expected_value(kernel[action], value, gamma)\n",
    "               for action, prob in policy.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=1.0):\n",
    "    value, delta = {state: 0. for state in env.P}, float(\"+inf\")\n",
    "    while delta > 1e-8:\n",
    "        # compute the operator\n",
    "        new = {state: expected_value(kernel, value, policy[state], gamma)\n",
    "               for state, kernel in env.P.items()}\n",
    "        \n",
    "        delta = max(abs(a - b) for a, b in zip(value.values(), new.values()))\n",
    "        value = new\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate a random exploration policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    state: {\n",
    "        action: 1. / env.nA for action in kernel\n",
    "    } for state, kernel in env.P.items()\n",
    "}\n",
    "\n",
    "value = policy_evaluation(env, policy, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, values = zip(*value.items())\n",
    "values = np.array(values).reshape(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(values, cmap=plt.cm.PuBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the $q$-function implied by the converged value function:\n",
    "$$\n",
    "Q(s, a)\n",
    "    = \\mathbb{E}_{z\\sim q(z\\mid s, a)} r(s, a, s') + \\gamma v^*(z)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fun = {\n",
    "    state: {\n",
    "        a: expected_value(k, value, 0.5) for a, k in kernel.items()\n",
    "    } for state, kernel in env.P.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the stationary $v$-function induces the following policy (consistent with it):\n",
    "$$\n",
    "\\pi(s)\n",
    "    = \\delta_{a_s}\n",
    "    \\,, \\text{ for } a_s = \\arg \\max_{a \\in A_s} Q(s, a) \n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_policy(q):\n",
    "    def _policy(env, state):\n",
    "        actions, expected = zip(*q[state].items())\n",
    "        return actions[np.argmax(expected)]\n",
    "\n",
    "    return _policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = [*run(env, get_greedy_policy(q_fun))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement and Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, *_ = zip(*episode)\n",
    "\n",
    "segs = np.c_[np.unravel_index(s, (8, 8))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.classic_control import MountainCarEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "\n",
    "class ModifiedMountainCarEnv(MountainCarEnv):\n",
    "    def __init__(self, goal_velocity = 0):\n",
    "        self.min_position = -2.5\n",
    "        self.max_position = 2.5\n",
    "        self.max_speed = 0.07\n",
    "        self.goal_position = 2.0\n",
    "        self.goal_velocity = goal_velocity\n",
    "\n",
    "        self.force = 0.001\n",
    "        self.gravity = 0.0025\n",
    "\n",
    "        self.low = np.array([self.min_position, -self.max_speed])\n",
    "        self.high = np.array([self.max_position, self.max_speed])\n",
    "\n",
    "        self.viewer = None\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
    "\n",
    "        self.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedMountainCarEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, agent, fps=15):\n",
    "    state, terminated = env.reset(), False\n",
    "    history = []\n",
    "    while not terminated:\n",
    "        state, reward, terminated, info = env.step(agent(state))\n",
    "        history.append((state, reward))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, terminated=False):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, state=None):\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (51, 71)\n",
    "state = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = (state - space.low) / (space.high - space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(object):\n",
    "    def __init__(self, space, shape):\n",
    "        assert isinstance(space, Box) and len(space.shape) == 1\n",
    "        assert space.is_bounded\n",
    "\n",
    "        if not isinstance(n_states, (list, tuple)):\n",
    "            shape = space.shape[0] * [shape]\n",
    "\n",
    "        assert shape == space.shape[0]\n",
    "        self.space, self.shape = space, shape\n",
    "\n",
    "    def to_ix(self, state, flatten=False):\n",
    "        unit = (state - space.low) / (space.high - space.low)\n",
    "        ix = (unit * shape + 0.5).astype(int)\n",
    "\n",
    "        if flatten:\n",
    "            return np.unravel_index(ix, shape=self.shape)\n",
    "        return ix\n",
    "\n",
    "    def from_ix(self, *index):\n",
    "        return np.array(index) * (space.high - space.low) + space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(obs, env):\n",
    "    space = env.observation_space\n",
    "    return (obs - space.low) / (space.high - space.low)\n",
    "\n",
    "class TabularQLearner(BaseAgent):\n",
    "    def __init__(self, env, n_states=51):\n",
    "        super().__init__(env)\n",
    "        self.n_states, self.n_actions = n_states, env.action_space.n\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q_table = torch.zeros(self.n_states, self.n_states, n_actions)\n",
    "\n",
    "    def __call__(self, state=None):\n",
    "        \n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
