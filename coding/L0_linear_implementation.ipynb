{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\ell_0$-linear layer implemenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost verbatim from [1712.01312](https://arxiv.org/abs/1712.01312.pdf)\n",
    "\n",
    "Efficient gradient based optimization of the expected $\\ell_0$ `norm` of parameter $\\theta$. Let $s$ be a continuous rv with $q(s\\mid \\phi)$. The binary gates $z$ are driven by\n",
    "$$\n",
    "    z = \\min\\bigl\\{\\max\\{s, 0\\}1, \\bigr\\}\n",
    "    \\,, s \\sim q(s\\big\\vert \\phi)\n",
    "    \\,, $$\n",
    "where the weights of a linear layer become $w = \\theta \\odot z$. This formulation allows the gate to be exactly zero, due to the underlying continueous rv $s$. Now\n",
    "$$ \\Pr\\bigl(z\\neq 0\\big\\vert \\phi\\bigr)\n",
    "    = \\int_0^{+\\infty} q(s\\big\\vert \\phi)ds\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized ERM thus becomes\n",
    "$$\n",
    "    \\mathbb{E}_{q(s\\mid \\phi)}\n",
    "        \\hat{\\mathbb{E}}_{x,y \\sim \\mathcal{S}}\n",
    "            \\ell(h(x; \\theta \\odot g(s)), y)\n",
    "        + \\tfrac\\lambda{\\lvert \\theta \\rvert}\n",
    "            \\sum_j \\Pr(z_j \\neq 0\\big\\vert \\phi_j)\n",
    "    \\,, $$\n",
    "for $g\\colon \\mathbb{R} \\to [0, 1] \\colon x \\mapsto \\min\\{1, \\max\\{x, 0\\}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the reparametrization trick on $s$:\n",
    "$$\n",
    "    q(\\cdot \\vert\\phi)\n",
    "        \\sim f(\\phi, \\varepsilon)\n",
    "        \\,, \\varepsilon \\sim p\n",
    "    \\,.$$\n",
    "The hard-concrete distribution does this trick. [concrete.pdf](http://www.stats.ox.ac.uk/~cmaddis/pubs/concrete.pdf) Which is a concrete binary rv passed thorugh a hard-sigmoid.\n",
    "\n",
    "So consider an rv $s\\in (0, 1)$ with pdf $q_s(s\\mid \\phi)$ and cdf $Q_s(s\\mid \\phi)$. The rv is parameterized by $\\log \\alpha$ -- the location, and $\\beta$ the temperature (recall the gumbel softmax trick -- it seems to be related). Let also $(\\gamma, \\zeta)$ be the parameters $\\gamma < 0 < \\zeta$, which strech the distribution of $s$ to $(\\gamma, \\zeta)$ interval.\n",
    "\n",
    "$$\n",
    "    u \\sim \\mathrm{U}(0, 1)\n",
    "    \\,, s = \\sigma_{\\tfrac1\\beta}(\\log \\tfrac{u}{1-u} + \\log \\alpha)\n",
    "    \\,, \\bar{s} = (\\zeta - \\gamma) s + \\gamma\n",
    "    \\,, z = g(\\bar{s})\n",
    "    \\,, $$\n",
    "where $\\sigma_\\beta\\colon x \\mapsto \\tfrac1{1+e^{-\\beta x}}$, and $s$ is concrete rv, $g$ is the hard-sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $t\\in (0, 1)$\n",
    "$$\n",
    "    \\{\\sigma_{\\tfrac1\\beta} \\leq t\\}\n",
    "        = \\{\n",
    "            \\tfrac{1-t}{t} \\leq e^{-\\tfrac1\\beta x}\n",
    "        \\}\n",
    "        = \\{\n",
    "            \\log\\tfrac{1-t}{t} \\leq -\\tfrac1\\beta x\n",
    "        \\}\n",
    "        = \\{\n",
    "            x \\leq \\beta\\log\\tfrac{t}{1-t}\n",
    "        \\}\n",
    "    \\,. $$\n",
    "\n",
    "For any $u \\in (0, 1)$\n",
    "$$\n",
    "    \\{u\\leq \\sigma(x)\\}\n",
    "        = \\{ 1+e^{-x} \\leq \\tfrac1u \\}\n",
    "        = \\{ -x \\leq \\log\\tfrac{1-u}u \\}\n",
    "        = \\{ \\log\\tfrac{u}{1-u} \\leq x \\}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus\n",
    "$$\\begin{align}\n",
    "    \\Pr\\bigl(z = 0\\big\\vert \\phi\\bigr)\n",
    "        &= \\Pr\\bigl(\\bar{s} \\leq 0 \\big\\vert \\phi\\bigr)\n",
    "        = \\Pr\\bigl(s \\leq \\tfrac{-\\gamma}{\\zeta - \\gamma} \\big\\vert \\phi\\bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\bigl(\n",
    "            \\log \\tfrac{u}{1-u} + \\log \\alpha\n",
    "                \\leq \\beta \\log \\tfrac{\\tfrac{-\\gamma}{\\zeta - \\gamma}}{1-\\tfrac{-\\gamma}{\\zeta - \\gamma}}\n",
    "        \\bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\bigl(\n",
    "            \\log \\tfrac{u}{1-u} \\leq \\beta \\log \\tfrac{-\\gamma}{\\zeta} - \\log \\alpha\n",
    "        \\bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\Bigl(\n",
    "            u \\leq \\sigma\\bigl(\\beta \\log \\tfrac{-\\gamma}{\\zeta} - \\log \\alpha\\bigr)\n",
    "        \\Bigr)\n",
    "        \\,.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $t \\in (0, 1)$:\n",
    "$$\\begin{align}\n",
    "    \\Pr\\bigl(z \\leq t \\big \\vert \\phi\\bigr)\n",
    "        &= \\Pr\\bigl( s \\leq \\tfrac{t - \\gamma}{\\zeta - \\gamma} \\big \\vert \\phi\\bigr)\n",
    "        = \\Pr\\bigl(\n",
    "            \\log \\tfrac{u}{1-u} + \\log \\alpha\n",
    "                \\leq \\beta \\log \\tfrac{\\tfrac{t - \\gamma}{\\zeta - \\gamma}}{1-\\tfrac{t - \\gamma}{\\zeta - \\gamma}}\n",
    "        \\bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\bigl(\n",
    "            \\log \\tfrac{u}{1-u}\n",
    "                \\leq \\beta \\log \\tfrac{t - \\gamma}{\\zeta - t}\n",
    "                    - \\log \\alpha\n",
    "        \\bigr)\n",
    "        = \\sigma\\bigl(\n",
    "            \\beta \\log \\tfrac{t - \\gamma}{\\zeta - t} - \\log \\alpha\n",
    "        \\bigr)\n",
    "        \\,.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\\begin{align}\n",
    "    \\Pr\\bigl(z \\neq 0\\big\\vert \\phi\\bigr)\n",
    "        &= 1 - \\Pr\\Bigl(\n",
    "            u \\leq \\sigma\\bigl(\\beta \\log \\tfrac{-\\gamma}{\\zeta} - \\log \\alpha\\bigr)\n",
    "        \\Bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\Bigl(\n",
    "            u \\geq \\sigma\\bigl(\\beta \\log \\tfrac{-\\gamma}{\\zeta} - \\log \\alpha\\bigr)\n",
    "        \\Bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\Bigl(\n",
    "            1-u \\leq 1-\\sigma\\bigl(\\beta \\log \\tfrac{-\\gamma}{\\zeta} - \\log \\alpha\\bigr)\n",
    "        \\Bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\Bigl(\n",
    "            1-u \\leq \\sigma\\bigl(\\log \\alpha - \\beta \\log \\tfrac{-\\gamma}{\\zeta}\\bigr)\n",
    "        \\Bigr)\n",
    "        \\\\\n",
    "        &= \\Pr\\Bigl(\n",
    "            u \\leq \\sigma\\bigl(\\log \\alpha - \\beta \\log \\tfrac{-\\gamma}{\\zeta}\\bigr)\n",
    "        \\Bigr)\n",
    "        \\\\\n",
    "        &= \\sigma\\bigl(\\log \\alpha - \\beta \\log \\tfrac{-\\gamma}{\\zeta}\\bigr)\n",
    "        \\,.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's carefully write the distirbution of $z$:\n",
    "$$\\begin{align}\n",
    "    \\Pr\\bigl(z \\leq t\\bigr)\n",
    "        &= 0\n",
    "        \\,, \\text{ for } t < 0\n",
    "        \\,, \\\\\n",
    "    \\Pr\\bigl(z \\leq t\\bigr)\n",
    "        &= \\sigma\\bigl(\n",
    "            \\beta \\log \\tfrac{t - \\gamma}{\\zeta - t} - \\log \\alpha\n",
    "        \\bigr)\n",
    "        \\,, \\text{ for } t \\in [0, 1)\n",
    "        \\,, \\\\\n",
    "    \\Pr\\bigl(z \\leq t\\bigr)\n",
    "        &= 1\n",
    "        \\,, \\text{ for } t \\geq 1\n",
    "        \\,.\n",
    "\\end{align}$$\n",
    "This distribution has atoms at $0$ and $1$:\n",
    "$$\\begin{align}\n",
    "    \\Pr\\bigl(z = 0\\bigr)\n",
    "        &= \\Pr\\bigl(z \\leq 0\\bigr) - \\Pr\\bigl(z < 0\\bigr)\n",
    "        = \\sigma\\bigl(\n",
    "            \\beta \\log \\tfrac{- \\gamma}\\zeta - \\log \\alpha\n",
    "        \\bigr) - \\lim_{t\\uparrow 0} \\Pr\\bigl(z \\leq t\\bigr)\n",
    "        = \\sigma\\bigl(\n",
    "            \\beta \\log \\tfrac{- \\gamma}\\zeta - \\log \\alpha\n",
    "        \\bigr)\n",
    "        \\,, \\\\\n",
    "    \\Pr\\bigl(z = 1\\bigr)\n",
    "        &= 1 - \\lim_{t\\uparrow 1} \\Pr\\bigl(z \\leq t\\bigr)\n",
    "        = 1 - \\sigma\\bigl(\n",
    "            \\beta \\log \\tfrac{1 - \\gamma}{\\zeta - 1} - \\log \\alpha\n",
    "        \\bigr)\n",
    "        = \\sigma\\bigl(\n",
    "            \\log \\alpha - \\beta \\log \\tfrac{1 - \\gamma}{\\zeta - 1}\n",
    "        \\bigr)\n",
    "        \\,.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $t\\in (0, 1)$ the pdf of $z$ is\n",
    "$$\n",
    "    \\tfrac{d}{dt} \\Pr(z \\leq t)\n",
    "        = \\tfrac{d}{dt} \\sigma\\bigl(\n",
    "            \\beta \\log \\tfrac{t - \\gamma}{\\zeta - t} - \\log \\alpha\n",
    "        \\bigr)\n",
    "        = \\sigma(x) \\sigma(-x) \\big\\vert_{x = \\beta \\log \\tfrac{t - \\gamma}{\\zeta - t} - \\log \\alpha}\n",
    "%         \\Bigl( \\beta \\tfrac1{t - \\gamma} + \\beta \\tfrac{1}{\\zeta - t} \\Bigr)\n",
    "%         = (\\ldots)\n",
    "        \\beta \\tfrac{\\zeta - \\gamma}{(t - \\gamma)(\\zeta - t)}\n",
    "%         \\beta (\\log{t - \\gamma} - \\log {\\zeta - t})\n",
    "%         \\tfrac{- e^{-x}}{(1+e^{-x})^2}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation of $z$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{\\mathrm{hc}} z\n",
    "        &= \\int z d\\mathbb{P}\n",
    "        = \\int_{\\{0\\}} + \\int_{(0, 1)} + \\int_{\\{1\\}}\n",
    "        = 0 \\sigma\\bigl(\n",
    "            \\beta \\log \\tfrac{- \\gamma}\\zeta - \\log \\alpha\n",
    "        \\bigr)\n",
    "        + 1 \\sigma\\bigl(\n",
    "            \\log \\alpha - \\beta \\log \\tfrac{1 - \\gamma}{\\zeta - 1}\n",
    "        \\bigr)\n",
    "        + \\int_{(0, 1)}\n",
    "            t \\tfrac{d}{dt} \\Pr(z \\leq t)\n",
    "        dt\n",
    "        \\\\\n",
    "        &= \\sigma\\bigl(\n",
    "            \\log \\alpha - \\beta \\log \\tfrac{1 - \\gamma}{\\zeta - 1}\n",
    "        \\bigr)\n",
    "        + \\sigma\\bigl(\n",
    "                \\beta \\log \\tfrac{1 - \\gamma}{\\zeta - 1} - \\log \\alpha\n",
    "            \\bigr)\n",
    "        - \\int_{(0, 1)}\n",
    "            \\sigma\\bigl(\n",
    "                \\beta \\log \\tfrac{t - \\gamma}{\\zeta - t} - \\log \\alpha\n",
    "            \\bigr) dt\n",
    "        \\\\\n",
    "        &= 1 - \\int_{(0, 1)}\n",
    "            \\sigma\\bigl(\n",
    "                \\beta \\log \\tfrac{t - \\gamma}{\\zeta - t} - \\log \\alpha\n",
    "            \\bigr) dt\n",
    "%         = \\int_0^1 \\Pr(z \\geq t) dt\n",
    "    \\,.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear transformtaion $y = Wx + b$ for $ W = \\theta \\odot z$, for $x\\in \\mathbb{R}^m$ and $y\\in \\mathbb{R}^n$. Then\n",
    "$$\\begin{align}\n",
    "    y_i &= b_i + \\sum_j \\theta_{ij} z_{ij} x_j\n",
    "        = b_i + \\sum_j z_{ij} e_i^\\top \\theta e_j x_j\n",
    "        \\\\\n",
    "        &= b_i + e_i^\\top \\mathop{diag}(z_i) \\theta x\n",
    "    \\,.\n",
    "\\end{align}\n",
    "$$\n",
    "Local reparametrization trick? The ICLR2018 paper uses a single sample (!) per minibatch. For otherwise we need `batch x n x m` samples, which is a lot.\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\partial y\n",
    "        &= \\partial W x + W \\partial x\n",
    "        = \\bigl( \\partial \\theta \\odot z + \\theta \\odot \\partial z \\bigr) x + W \\partial x\n",
    "        \\\\\n",
    "        &= \\bigl( \\partial \\theta \\odot z + \\theta \\odot \\partial z \\bigr) x + W \\partial x\n",
    "        \\,.\n",
    "\\end{align}$$\n",
    "\n",
    "See the last paragraph before section 3 of their paper! There they say that they use group dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear layer $\\mathbb{R}^n \\to \\mathbb{R}^m$\n",
    "$$\n",
    "    y = W x + b\n",
    "    \\,, W_{ij} = \\theta_{ij} z_{ij}\n",
    "\\,, $$\n",
    "\n",
    "where $z \\in [0, 1]^{n\\times m}$ is a learnable variational dropout mask\n",
    "\n",
    "$$\n",
    "    u_{ij} \\sim \\mathrm{U}(0, 1)\n",
    "    \\,, s_{ij} = \\sigma\\bigl(\\tfrac1\\beta (\\log \\tfrac{u_{ij}}{1-u_{ij}} + \\log \\alpha_{ij})\\bigr)\n",
    "    \\,, \\\\\n",
    "    z_{ij} = \\min\\bigl\\{1, \\max\\{0, (\\zeta - \\gamma) s_{ij} + \\gamma\\}\\bigr\\}\n",
    "    \\,, \\sigma\\colon x \\mapsto \\tfrac1{1+e^{-x}}\n",
    "    \\,.\n",
    "$$\n",
    "\n",
    "$\\gamma=-0.1, \\zeta=+1.1$, $\\beta = 0.66$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective\n",
    "$$\n",
    "    \\tfrac1{2B} \\sum_b \\|g(x_b) - y_b \\|_2^2\n",
    "        + \\lambda \\tfrac1{\\# \\text{par}}\n",
    "            \\sum_{ij} \\Pr(z_{ij} \\neq 0)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cplxmodule.relevance import named_penalties, penalties\n",
    "\n",
    "from torch.nn import Linear\n",
    "from cplxmodule.relevance import LinearARD\n",
    "from cplxmodule.relevance import LinearL0ARD, LinearLASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 250, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.r_ if True else np.random.permutation(shape[0])\n",
    "index = index[:shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ard = LinearL0ARD(*shape, bias=False, reduction=\"mean\", group=None)  # longer to learn on small data, noisy grads indeed\n",
    "# model_ard = LinearARD(*shape, bias=False)\n",
    "# model_ard = Linear(*shape, bias=False)\n",
    "\n",
    "# model_ard = LinearLASSO(*shape, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxmodule.masked import LinearMasked\n",
    "\n",
    "model_masked = LinearMasked(*shape, model_ard.bias is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on a simple identiy mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_ard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "x = torch.randn(100, shape[0])\n",
    "y = -x[:, index]\n",
    "\n",
    "with tqdm.tqdm(range(8000)) as bar:\n",
    "    for epoch in bar:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = F.mse_loss(model(x), y)\n",
    "        loss += sum(penalties(model))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        losses.append(float(loss))\n",
    "    # end for\n",
    "# end with\n",
    "\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(penalties(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxmodule.relevance import sparsity\n",
    "\n",
    "import math\n",
    "\n",
    "tau = 0.5\n",
    "\n",
    "sparsity(model, threshold=tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpd.tools.utils import sparsity_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*sparsity_details(model, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = []\n",
    "for _ in tqdm.tqdm(range(1000)):\n",
    "    x = torch.randn(100, shape[0])\n",
    "    y = -x[:, index]\n",
    "\n",
    "    test_loss.append(float(F.mse_loss(model(x), y)))\n",
    "\n",
    "plt.hist(test_loss, bins=51);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxmodule.masked import is_sparse\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n",
    "\n",
    "with torch.no_grad():\n",
    "    if \"LinearL0ARD\" in globals() and isinstance(model, LinearL0ARD):\n",
    "        z = model.gate(None).detach()\n",
    "        ax[0].imshow(z.numpy(), cmap=plt.cm.bone)\n",
    "        ax[0].set_title(r\"Gate $z_{ij}$\")\n",
    "\n",
    "    elif is_sparse(model):\n",
    "        z = model.mask\n",
    "        ax[0].imshow(z.numpy(), cmap=plt.cm.bone)\n",
    "        ax[0].set_title(r\"Gate $z_{ij}$\")\n",
    "    \n",
    "    ax[1].imshow(abs(getattr(model, \"weight_masked\", model.weight)).numpy(), cmap=plt.cm.binary_r)\n",
    "    ax[1].set_title(r\"Absolute learnt weight $| \\theta_{ij} |$\")\n",
    "\n",
    "    if isinstance(model, LinearMasked):\n",
    "        relevance = 1 - model.mask\n",
    "    else:\n",
    "        relevance = model.log_alpha\n",
    "    ax[2].imshow(relevance.detach().numpy(), cmap=plt.cm.bone_r)\n",
    "    ax[2].set_title(r\"Relevance $\\log \\alpha_{ij}$ / mask\")\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight[np.r_[:shape[1]], index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxmodule.masked import compute_ard_masks, deploy_masks\n",
    "\n",
    "masks = compute_ard_masks(model_ard, threshold=0)\n",
    "\n",
    "model_masked = deploy_masks(model_masked, state_dict=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*model_masked.named_buffers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_masked.weight_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [*model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_masked.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxmodule.relevance.base import named_relevance, named_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpd.models import ModelHUAWEI\n",
    "from dpd.models.huawei import ModelHUAWEIBoosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = ModelHUAWEI.default_recipe(32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ModelHUAWEI(*recipe, n_columns=1, ard=True)\n",
    "model = ModelHUAWEIBoosted(*recipe, n_columns=1, linear=LinearARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model[\"boost00\"][0][\"dense00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*sparsity_details(model, 4.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cplxmodule.relevance import named_penalties, penalties\n",
    "from cplxmodule.relevance.base import BaseARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearL0ARD(torch.nn.Linear, BaseARD):\n",
    "    \"\"\"L0 regularized linear layer according to [1]_.\n",
    "    \n",
    "    Details\n",
    "    -------\n",
    "    IMPORTANT: This implemetnation use -ve log-alpha parametrization\n",
    "    in order to keep the layer's parameters interpretation aligned \n",
    "    with the interpretation in variational dropout layer of Kingma\n",
    "    et al. (relevance.LinearARD).\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    [1]_ :: https://arxiv.org/abs/1712.01312.pdf\n",
    "    [2]_ :: https://arxiv.org/abs/1902.09574.pdf\n",
    "    \"\"\"\n",
    "    beta, gamma, zeta = .25, -0.1, 1.1\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.log_alpha = Parameter(\n",
    "            torch.Tensor(*self.weight.shape))\n",
    "        self.reset_variational_parameters()\n",
    "\n",
    "    def reset_variational_parameters(self):\n",
    "        # assume everything is important (but do not saturate the sigmoid too much)\n",
    "        self.log_alpha.data.uniform_(-0.45, -0.45)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if not self.training:\n",
    "            # suppose u = 0.5, the the input is zero\n",
    "            z = self.sample_hc(0.0, beta=self.beta)  # 1.0 in paper eq(13), self.beta in code\n",
    "            return F.linear(input, self.weight * z, self.bias)\n",
    "\n",
    "        # a single mask sample for the whole batch!\n",
    "        u = torch.rand_like(self.log_alpha)\n",
    "        z = self.sample_hc(torch.log(u) - torch.log(1 - u), beta=self.beta)\n",
    "        return F.linear(input, self.weight * z, self.bias)\n",
    "\n",
    "    def sample_hc(self, logit=0.0, beta=1.0):\n",
    "        s = torch.sigmoid((logit - self.log_alpha) / beta)\n",
    "        return torch.clamp((self.zeta - self.gamma) * s + self.gamma, 0, 1)\n",
    "    \n",
    "    @property\n",
    "    def penalty(self):\n",
    "        shift = self.beta * math.log(- self.gamma / self.zeta)\n",
    "        return 1 - torch.sigmoid(self.log_alpha + shift).mean()\n",
    "\n",
    "    def get_sparsity_mask(self, threshold):\n",
    "        r\"\"\"Get the dropout mask based on the log-relevance.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return torch.ge(self.log_alpha, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearL0_arxiv171201312(torch.nn.Linear, BaseARD):\n",
    "    \"\"\"L0 regularized linear layer according to [1]_.\"\"\"\n",
    "    beta, gamma, zeta = .5, -0.1, 1.1\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.log_alpha = Parameter(torch.Tensor(*self.weight.shape))\n",
    "        self.reset_variational_parameters()\n",
    "\n",
    "    def reset_variational_parameters(self):\n",
    "        # 0.6 = a / (a+1) = n / (n + m), for a = n/m, n=2, m=3. put n, m = 9, 11\n",
    "        self.log_alpha.data.normal_(0.45, std=0.01)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if not self.training:\n",
    "            s = torch.sigmoid(self.log_alpha)\n",
    "            z = torch.clamp((self.zeta - self.gamma) * s + self.gamma, 0, 1)\n",
    "            return F.linear(input, self.weight * z, self.bias)\n",
    "\n",
    "        # a single mask sample for the whole batch!\n",
    "        u = torch.rand_like(self.log_alpha)\n",
    "        logit = torch.log(u) - torch.log(1 - u) + self.log_alpha\n",
    "        s = torch.sigmoid(logit / self.beta)\n",
    "        z = torch.clamp((self.zeta - self.gamma) * s + self.gamma, 0, 1)\n",
    "        return F.linear(input, self.weight * z, self.bias)\n",
    "\n",
    "    @property\n",
    "    def penalty(self):\n",
    "        shift = self.beta * math.log(- self.gamma / self.zeta)\n",
    "        return torch.sigmoid(self.log_alpha - shift).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 50) * 10\n",
    "x1 = F.hardtanh(x, 0., 1.)\n",
    "x2 = torch.clamp(x, 0., 1.)\n",
    "\n",
    "assert torch.allclose(x2, x1)\n",
    "assert torch.allclose(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxmodule.relevance.base import BaseARD\n",
    "from cplxmodule.relevance.utils import torch_sparse_linear, torch_sparse_tensor\n",
    "from cplxmodule.relevance.utils import parameter_to_buffer, buffer_to_parameter\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class Dummy(torch.nn.Linear):\n",
    "    @property\n",
    "    def is_sparse(self):\n",
    "        mode = getattr(self, \"sparsity_mode_\", None)\n",
    "        return mode is not None\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.is_sparse:\n",
    "            return self.forward_sparse(input)\n",
    "        return super().forward(input)\n",
    "\n",
    "    def forward_sparse(self, input):\n",
    "        if self.sparsity_mode_ == \"dense\":\n",
    "            weight = self.weight_ * self.nonzero_\n",
    "            return F.linear(input, weight, self.bias)\n",
    "\n",
    "        else:\n",
    "            weight = torch_sparse_tensor(self.nonzero_, self.weight_,\n",
    "                                         self.weight.shape)\n",
    "            return torch_sparse_linear(input, weight, self.bias)\n",
    "\n",
    "    def sparsify(self, threshold=1.0, mode=\"dense\"):\n",
    "        if mode is not None and mode not in (\"dense\", \"sparse\"):\n",
    "            raise ValueError(f\"\"\"`mode` must be either 'dense', 'sparse' or \"\"\"\n",
    "                             f\"\"\"`None` (got '{mode}').\"\"\")\n",
    "\n",
    "        if mode is not None:\n",
    "            with torch.no_grad():\n",
    "                mask = torch.gt(abs(self.weight), threshold)\n",
    "\n",
    "            if mode == \"sparse\":\n",
    "                # truly sparse mode\n",
    "                weight = self.weight.data[mask].clone()\n",
    "                self.register_buffer(\"nonzero_\", mask.nonzero().t())\n",
    "\n",
    "            elif mode == \"dense\":\n",
    "                # smiluated sparse mode\n",
    "                mask = mask.to(self.weight).data\n",
    "                weight = self.weight.data * mask\n",
    "                self.register_buffer(\"nonzero_\", mask)\n",
    "\n",
    "            # make weight into a buffer (load_state dict doesn't care\n",
    "            #  about param/buffer distinction!)\n",
    "            self.register_parameter(\"weight_\", torch.nn.Parameter(weight))\n",
    "            parameter_to_buffer(self, \"weight\")\n",
    "\n",
    "        elif self.is_sparse:\n",
    "            # reinstate the weight as the parameter and delete runtime stuff\n",
    "            del self.nonzero_, self.weight_\n",
    "            buffer_to_parameter(self, \"weight\")\n",
    "\n",
    "        # end if\n",
    "\n",
    "        self.sparsity_mode_ = mode\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cplxmodule.layers import CplxParameter, CplxLinear\n",
    "from cplxmodule import Cplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_to_buffer(module, name):\n",
    "    # par could be a solo parameter or a container (essentially a submodule)\n",
    "    par = getattr(module, name)\n",
    "    if isinstance(par, (torch.nn.ParameterDict, torch.nn.ParameterList)):\n",
    "        # parameter containers no not use buffers and aren't expected to.\n",
    "        #  So we hide parameters there. This precludes acces via __getitem__\n",
    "        #  though. Not via __getattr__\n",
    "\n",
    "        # create a copy of the container's master parameter dict's keys and mutate\n",
    "        for name in list(par._parameters):\n",
    "            # By design of Parameter containers this never recurses deeper\n",
    "            parameter_to_buffer(par, name)\n",
    "        return\n",
    "\n",
    "    # a solo parameter\n",
    "    if par is not None and not isinstance(par, torch.nn.Parameter):\n",
    "        raise KeyError(f\"parameter '{name}' is not a tensor.\")\n",
    "\n",
    "    # remove the parameter and mutate into a grad-detached buffer\n",
    "    delattr(module, name)\n",
    "    par = par.detach() if par is not None else None\n",
    "    module.register_buffer(name, par)\n",
    "\n",
    "def buffer_to_parameter(module, name):\n",
    "    # a buffer here can be a buffer or a former mutated parameter container\n",
    "    buf = getattr(module, name)\n",
    "    if isinstance(buf, (torch.nn.ParameterDict, torch.nn.ParameterList)):\n",
    "        # create a copy of the container's master buffer dict's keys and restore\n",
    "        for name in list(buf._buffers):\n",
    "            # By design of Parameter containers this never goes deeper\n",
    "            #  than this call\n",
    "            buffer_to_parameter(buf, name)\n",
    "        return\n",
    "\n",
    "    if buf is not None and not isinstance(buf, torch.Tensor):\n",
    "        raise KeyError(f\"buffer '{name}' is not a tensor.\")\n",
    "\n",
    "    # remove the buffer and mutate back into a proper parameter\n",
    "    delattr(module, name)\n",
    "    buf = torch.nn.Parameter(buf) if buf is not None else None\n",
    "    module.register_parameter(name, buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = torch.nn.Module()\n",
    "mod.par = torch.nn.Parameter(torch.randn(10))\n",
    "mod.par_d = torch.nn.ParameterDict({\n",
    "    \"par_1\": torch.nn.Parameter(torch.randn(10)),\n",
    "    \"par_2\": torch.nn.Parameter(torch.randn(10)),\n",
    "})\n",
    "mod.par_l = torch.nn.ParameterList([\n",
    "    torch.nn.Parameter(torch.randn(10)),\n",
    "    torch.nn.Parameter(torch.randn(10)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = CplxLinear(10, 10)\n",
    "\n",
    "lin.test = torch.nn.ParameterList([\n",
    "    torch.nn.Parameter(lin.weight.real.clone()),\n",
    "    None,\n",
    "    torch.nn.Parameter(lin.weight.real.clone())\n",
    "])\n",
    "\n",
    "lin.test_2 = torch.nn.ParameterDict({\n",
    "    \"a\": torch.nn.Parameter(torch.randn(10)),\n",
    "    \"z\": torch.nn.Parameter(torch.randn(10)),\n",
    "    \"_12omega\": None,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = Cplx(**lin.weight)\n",
    "\n",
    "parameter_to_buffer(lin, \"weight\")\n",
    "print([n for n, b in lin.named_buffers()], list(lin.weight._buffers))\n",
    "\n",
    "buffer_to_parameter(lin, \"weight\")\n",
    "print([n for n, b in lin.named_buffers()], list(lin.weight._buffers))\n",
    "\n",
    "assert np.allclose(Cplx(**lin.weight).detach().numpy(), before.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = [*lin.test]\n",
    "\n",
    "parameter_to_buffer(lin, \"test\")\n",
    "print([n for n, b in lin.named_buffers()], list(lin.test._buffers))\n",
    "\n",
    "buffer_to_parameter(lin, \"test\")\n",
    "print([n for n, b in lin.named_buffers()], list(lin.test._buffers))\n",
    "\n",
    "assert all(a is None and b is None or torch.allclose(a, b)\n",
    "           for a, b in zip(lin.test, copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = {**lin.test_2}\n",
    "\n",
    "parameter_to_buffer(lin, \"test_2\")\n",
    "print([n for n, b in lin.named_buffers()], list(lin.test_2._buffers))\n",
    "\n",
    "buffer_to_parameter(lin, \"test_2\")\n",
    "print([n for n, b in lin.named_buffers()], list(lin.test_2._buffers))\n",
    "\n",
    "assert all(a is None and b is None or torch.allclose(a, b)\n",
    "           for a, b in zip(lin.test_2.values(), copy.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import expit\n",
    "\n",
    "beta, gamma, zeta = 0.95, -0.5, 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = np.meshgrid(np.linspace(-.25, 1.25, num=501),\n",
    "                   np.linspace(-12, 12, num=101))\n",
    "\n",
    "t, log_a = mesh\n",
    "\n",
    "tcl = np.clip(t, 0, 1)\n",
    "cdf = expit(beta * (np.log(tcl - gamma) - np.log(zeta - tcl)) - log_a)\n",
    "cdf[t >= 1.] = 1.\n",
    "cdf[t < 0] = 0.\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(t, log_a, cdf)\n",
    "\n",
    "ax.view_init(35, 120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slices of CDF at large $\\log \\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_a[-50:-35, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0][-90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(t[0], np.diff(cdf, axis=1, prepend=0).T)\n",
    "plt.plot(t[0], cdf[-50:-35].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(t[0], np.diff(cdf, axis=1, prepend=0).T)\n",
    "plt.plot(log_a.T[0], cdf.T[-90:-84].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p \\mapsto \\log \\alpha$ such that\n",
    "$$\n",
    "    p = \\Pr(z=1)\n",
    "        = \\sigma\\bigl( \\log \\alpha - \\beta \\log\\tfrac{1-\\gamma}{\\zeta- 1}\\bigr)\n",
    "    \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "\n",
    "eps = 1\n",
    "beta, gamma, zeta = 0.5, -0-eps, 1.+eps\n",
    "\n",
    "p = np.linspace(0.0, 1.0, num=101)[1:-1]\n",
    "plt.plot(p, logit(p) + beta * (math.log(1 - gamma) - math.log(zeta - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5, -0.15, 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, zeta, gamma = .1, 1.25, -0.25\n",
    "\n",
    "la = np.linspace(-8, 8, num=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.random.rand(1001, 10000)\n",
    "z = np.log(u) - np.log(1-u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = expit((z-la[:, np.newaxis])/beta).mean(axis=-1)\n",
    "pp = expit(-la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mc-pp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
