{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the implicit gradient w.r.t hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementary theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\\mathbb{R}$-normed spaces $(E, \\|\\cdot\\|)$ and $(F, \\|\\cdot\\|)$,\n",
    "and a map $\\phi\\colon U \\to F$, where $U$ is open set in $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map $\\phi$ is Frechet differentiadle at $a\\in U$ if there exists a\n",
    "bounded linear map $\\ell\\colon E \\to F$ such that for any $\\varepsilon > 0$\n",
    "there is $\\delta > 0$ such that for any $\\|h\\| \\leq \\delta$\n",
    "with $a+h \\in U$ we have\n",
    "$$\n",
    "    \\bigl\\|\n",
    "        \\phi(a+h) - \\phi(a)\n",
    "        - \\ell(h)\n",
    "    \\bigr\\| \\leq \\varepsilon \\|h\\|\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This map is unique and called the differential of $\\phi$ at $a$, i.e. $d\\phi(a) = \\ell$.\n",
    "\n",
    "Note that, since $U$ is open (in the norm topology) and $a\\in U$ we can\n",
    "always choose a smaller $\\delta$ so that for all $\\|h\\|\\leq \\delta$ it\n",
    "holds that $a + h \\in U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an operator, $d\\phi\\colon U \\to \\mathcal{B}(E, F)$, is a mapping\n",
    "from the open domain of $\\phi$ into the space of linear operators.\n",
    "Note that the space of linear operators $\\mathcal{B}(E, F)$ is also\n",
    "normed: for $\\ell \\in \\mathcal{B}(E, F)$ we have\n",
    "$$\n",
    "    \\|\\ell\\|\n",
    "        = \\sup\\bigl\\{\n",
    "            \\tfrac{\\|\\ell x\\|}{\\|x\\|}\n",
    "            \\colon x \\in E\\,,\\,x\\neq 0\n",
    "        \\bigr\\}\n",
    "        = \\sup\\bigl\\{\n",
    "            \\|\\ell x\\|\n",
    "            \\colon x \\in E\\,,\\,\\|x\\|\\leq 1\n",
    "        \\bigr\\}\n",
    "        = \\inf\\bigl\\{\n",
    "            K \\geq 0\n",
    "            \\colon \\|\\ell x \\| \\leq K \\|x\\|\\,,\\, \\forall x\\in E\n",
    "        \\bigr\\}\n",
    "    \\,. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore applying the definition to $a \\mapsto d\\phi(a)$, we get\n",
    "that the second order differential of $\\phi$, aka the differential\n",
    "of $a\\mapsto d\\phi(a)$, denoted by $d^2\\phi$ at $a$ is an element\n",
    "of $\\mathcal{B}(E, \\mathcal{B}(E, F))$, i.e. a bilinear map. The \n",
    "definition goes: $\\phi$ is twice differentiable at $a$ if $\\phi$\n",
    "is differentiable on $U$ and the map $d\\phi$ is differentiable as\n",
    "$a$, i.e. for any $\\varepsilon > 0$ there is $\\delta > 0$ such that\n",
    "for all $\\|h\\| \\leq \\delta$ such that $a + h \\in U \\cap E$ we have\n",
    "\n",
    "$$\n",
    "\\bigl\\|\n",
    "    d\\phi(a+h) - d\\phi(a)\n",
    "    - d(d\\phi(a))(h)\n",
    "\\bigr\\| \\leq \\varepsilon \\|h\\|\n",
    "    \\,. $$\n",
    "\n",
    "The linear map $d(d\\phi(a))$ takes linear maps as values, and so\n",
    "$h\\mapsto d(d\\phi(a))(h)$ is shortened to $h\\mapsto d^2\\phi(a)(h, \\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is to notice that for any fixed $h\\in E$ we can\n",
    "put $\\psi_h \\colon U \\to F \\colon a\\mapsto d\\phi(a)(h)$.\n",
    "Then $d\\psi_h(a)$ is $d^2\\phi(a)(\\cdot, h)$.\n",
    "\n",
    "Indeed fix some $h \\in E$. Then, formally, for any $\\varepsilon > 0$\n",
    "there exists $\\delta > 0$ such that for any $\\|v\\| \\leq \\delta$ with\n",
    "$a+v \\in U\\cap E$ we have the linearization for $\\psi_h(a)$ and\n",
    "$d\\phi(a)$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\bigl\\|\n",
    "        d\\psi_h(a)(v) - d^2\\phi(a)(v, h)\n",
    "    \\bigr\\|\n",
    "        &\\leq \\bigl\\|\n",
    "            d\\phi(a + v)(h) - d\\phi(a)(h) - d^2\\phi(a)(v, h)\n",
    "        \\bigr\\| + \\bigl\\|\n",
    "            d\\phi(a + v)(h) - d\\phi(a)(h) - d\\psi_h(a)(v)\n",
    "        \\bigr\\|\n",
    "        \\\\\n",
    "        &\\leq \\bigl\\|\n",
    "            d\\phi(a + v) - d\\phi(a) - d^2\\phi(a)(v, \\cdot)\n",
    "        \\bigr\\| \\| h \\| + \\bigl\\|\n",
    "            \\psi_h(a + v) - \\psi_h(a) - d\\psi_h(a)(v)\n",
    "        \\bigr\\|\n",
    "        \\\\\n",
    "        &\\leq \\varepsilon \\|v\\| (1 + \\| h \\|)\n",
    "\\end{align}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for any $\\|x\\| \\leq 1$ we have $\\|\\delta x\\|\\leq \\delta$ and\n",
    "\n",
    "$$\n",
    "\\bigl\\|\n",
    "    d\\psi_h(a)(x) - d^2\\phi(a)(x, h)\n",
    "\\bigr\\|\n",
    "    = \\tfrac1\\delta \\bigl\\|\n",
    "        d\\psi_h(a)(\\delta x) - d^2\\phi(a)(\\delta x, h)\n",
    "    \\bigr\\|\n",
    "    \\leq \\tfrac1\\delta \\varepsilon (1 + \\| h \\|) \\|\\delta x \\|\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\delta > 0$ the term $\\varepsilon (1 + \\| h \\|)$ upper bounds\n",
    "the norm for all $\\|x\\| \\leq 1$, and thus the operator norm is also\n",
    "bounded. Therefore for any $\\varepsilon > 0$ we have\n",
    "\n",
    "$$\n",
    "\\bigl\\|\n",
    "    d\\psi_h(a) - d^2\\phi(a)(\\cdot, h)\n",
    "\\bigr\\|\n",
    "    \\leq \\varepsilon (1 + \\| h \\|)\n",
    "    \\,, $$\n",
    "which implies that $d\\psi_h(a) = d^2\\phi(a)(\\cdot, h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(duh)**\n",
    "This implies that we cat take $a\\mapsto d\\phi(a)$, evaluate it at\n",
    "some constant vector $h$ to get $a\\mapsto d\\phi(a)(h)$, and\n",
    "differentiale. In $\\mathbb{R}^n$ this means that differentiating\n",
    "$a \\mapsto \\nabla \\phi(a)^\\top h$ yields $\\nabla^2 \\phi(a) h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $\\nabla^2_{\\lambda\\omega} f(\\omega, \\lambda) v$ do:\n",
    "1. fix $v$ and compute $\n",
    "\\phi_v\n",
    "\\colon (\\omega, \\lambda) \\mapsto\n",
    "    \\nabla_\\omega f(\\omega, \\lambda)^\\top v\n",
    "$\n",
    "2. compute $\n",
    "\\nabla_\\lambda \\phi_v(\\omega, \\lambda)\n",
    "    = \\nabla_\\lambda \\bigl( \\nabla_\\omega f(\\omega, \\lambda)^\\top v \\bigr)\n",
    "    = \\nabla^2_{\\lambda\\omega} f(\\omega, \\lambda) v\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear operatons in the direct-product space of inner product spaces:\n",
    "$( E, [\\cdot, \\cdot])$ with $E = \\prod_i E_i$ and $\n",
    "[\\cdot, \\cdot]\n",
    "    = \\oplus_i \\langle \\cdot, \\cdot \\rangle_i\n",
    "$\n",
    "with each $(E_i, \\langle \\cdot, \\cdot \\rangle_i)$ being an inner product\n",
    "space over $\\mathbb{K}$:\n",
    "\n",
    "* (`daxpy`, `dscal`) $a, b \\in E$, and $\\lambda \\in \\mathbb{K}$ we have $\\lambda a + b \\in E$\n",
    "given by $(\\lambda a + b)_i = \\lambda a_i + b_i$ in each $E_i$.\n",
    "\n",
    "* (`ddot`, `dnorm`) for $a, b \\in E$ we have $[a, b] = \\sum_i \\langle a_i, b_i \\rangle_i$\n",
    "and $\\|a\\| = \\sqrt{[a, a]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.trcg import clone, dzero, daxpy, ddot\n",
    "\n",
    "from tools.trcg import trcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "X = torch.randn(1024, 10).double()\n",
    "y = X @ torch.randn(10, 1).double()\n",
    "\n",
    "train = torch.utils.data.TensorDataset(X[:25], y[:25])\n",
    "test = torch.utils.data.TensorDataset(X[250:], y[250:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.tensor(1., requires_grad=True).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about within-model hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(model, X, y):\n",
    "    l2_reg = sum(p.norm()**2 for n, p in model.named_parameters()\n",
    "                 if \"bias\" not in n)\n",
    "    return 0.5 * F.mse_loss(model(X), y) + 0.5 * alpha * l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, LeakyReLU, Tanh\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(10, 64),\n",
    "    LeakyReLU(),\n",
    "    Linear(64, 64),\n",
    "    LeakyReLU(),\n",
    "    Linear(64, 1),\n",
    "#     Linear(10, 1, bias=False)\n",
    ").double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization with approximate gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "* at $\\omega^*(\\lambda) = \\arg\\min_\\omega \\mathcal{L}_\\mathrm{train}(\\omega, \\lambda)$ we have $\\nabla_\\omega \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda) \\equiv 0$, i.e.\n",
    "\n",
    "$$\n",
    "    \\nabla_\\lambda \\omega^*(\\lambda) \n",
    "        \\nabla^2_{\\omega\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "    + \\nabla^2_{\\lambda\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "        = 0\n",
    "    \\,, $$\n",
    "\n",
    "\n",
    "* using the chain rule we get for $\n",
    "F\\colon \\lambda \\mapsto \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "$\n",
    "\n",
    "$$\n",
    "    \\nabla_\\lambda F(\\lambda)\n",
    "        = \\nabla_\\lambda \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "        + \\nabla_\\lambda \\omega^*(\\lambda)\n",
    "            \\, \\nabla_\\omega \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HOAG](https://arxiv.org/pdf/1602.02355.pdf) Algorithm:\n",
    "1. (approx) solve $\\omega^*(\\lambda) = \\arg\\min \\mathcal{L}_\\mathrm{train}(\\omega, \\lambda)$\n",
    "2. (approx) find such $q_\\lambda$ that\n",
    "$\n",
    "    \\nabla^2_{\\omega\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "        \\, q_\\lambda\n",
    "    = \\nabla_\\omega \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "$ (can add $c I_\\omega$ for $0 < c \\ll 1$ to the hessian so stabilize CG)\n",
    "3. compute\n",
    "$$\n",
    "    \\nabla_\\lambda F(\\lambda)\n",
    "        = \\nabla_\\lambda \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "        - \\nabla^2_{\\lambda\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda) \\, q_\\lambda\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore \n",
    "\n",
    "$$\n",
    "    \\nabla_\\lambda F(\\lambda)\n",
    "        = \\nabla_\\lambda \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "        - \\nabla^2_{\\lambda\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "        \\underbrace{\n",
    "            \\bigl(\n",
    "                \\nabla^2_{\\omega\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "            \\bigr)^{-1}\n",
    "            \\nabla_\\omega \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "        }_{q_\\lambda}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (approx) find stationary point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\omega^*(\\lambda)$ sits in `model.parameters()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)**\n",
    "we have to train the model to $\n",
    "\\nabla_\\omega \\mathcal{L}_\\mathrm{train}\n",
    "    (\\omega^*(\\lambda), \\lambda)\n",
    "    \\approx 0\n",
    "$\n",
    "(almost zero gradient), and preferably to a local minimum (so that CG\n",
    "below uses a positive definite hessian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl import fit\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "# 1. find (approx) \\arg \\min_\\omega L_train(\\omega, \\lambda)\n",
    "fit(model, train, criterion=loss, batch_size=32, n_epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_h = loss(model, *train.tensors)  # L_train(\\omega^*(\\lambda), \\lambda)\n",
    "grad_h_omega = torch.autograd.grad(loss_h, model.parameters(), create_graph=False)\n",
    "\n",
    "sum(map(torch.norm, grad_h_omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. (approx) find $q_\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe \n",
    "$$\n",
    "    \\nabla^2_{\\omega\\omega}\n",
    "        h(\\omega, \\lambda) \\delta\n",
    "        = \\tfrac{\\partial}{\\partial \\omega}\n",
    "            \\bigl\\{ \\nabla_\\omega^\\top h(\\omega, \\lambda) \\delta \\bigr \\}\n",
    "        = \\tfrac{\\partial}{\\partial \\omega} \\ell_\\delta(\\omega)\n",
    "        = d\\bigl\\{ dh(\\omega, \\lambda)(\\delta) \\bigr \\}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 create a hess-vect closure\n",
    "def get_hesv_op(model, dataset):\n",
    "    # get \\nabla_\\omega L_train(\\omega^*(\\lambda), \\lambda)\n",
    "    loss_h = loss(model, *dataset.tensors)\n",
    "    grad_h_omega = torch.autograd.grad(loss_h, model.parameters(), create_graph=True)\n",
    "\n",
    "    def _closure(delta, nugget=1e-6):  # adding a tiny diagonal helps a lot\n",
    "        with torch.enable_grad():\n",
    "            # \\omega \\mapsto \\nabla_\\omega^\\top h(\\omega, \\lambda) \\delta\n",
    "            grad_h_vect = ddot(grad_h_omega, delta)\n",
    "\n",
    "            # get \\nabla^2_{\\omega\\omega} L_train(\\omega^*(\\lambda), \\lambda) \\delta\n",
    "            hesv_h_omega = torch.autograd.grad(grad_h_vect, model.parameters(), retain_graph=True)\n",
    "\n",
    "            return daxpy(nugget, delta, hesv_h_omega)\n",
    "\n",
    "    return _closure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test loss and find the omega-gradient at the star-point $\\omega^*(\\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_g = loss(model, *test.tensors)  # L_test(\\omega^*(\\lambda), \\lambda)\n",
    "\n",
    "grad_g_omega = torch.autograd.grad(loss_g, model.parameters(), create_graph=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve $\n",
    "\\nabla^2_{\\omega\\omega}\n",
    "    \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "    \\,\\, \\mathbf{x}\n",
    "    = \\nabla_\\omega \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ax = get_hesv_op(model, train)  # lambda v: [-p for p in v]\n",
    "\n",
    "x, r = dzero(clone(grad_g_omega)), clone(grad_g_omega)\n",
    "print(trcg(Ax, r, x, rtol=1e-8, atol=1e-9, verbose=True))\n",
    "\n",
    "assert all(map(torch.allclose, Ax(x), grad_g_omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the gradinet norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_h = loss(model, *train.tensors)  # L_train(\\omega^*(\\lambda), \\lambda)\n",
    "grad_h_omega = torch.autograd.grad(loss_h, model.parameters(), create_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(map(torch.norm, grad_h_omega)), sum(map(torch.norm, grad_g_omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (approx) find the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "    \\nabla_\\lambda F(\\lambda)\n",
    "        = \\nabla_\\lambda \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "        - \\nabla^2_{\\lambda\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "        \\underbrace{\n",
    "            \\bigl(\n",
    "                \\nabla^2_{\\omega\\omega} \\mathcal{L}_\\mathrm{train}(\\omega^*(\\lambda), \\lambda)\n",
    "            \\bigr)^{-1}\n",
    "            \\nabla_\\omega \\mathcal{L}_\\mathrm{test}(\\omega^*(\\lambda), \\lambda)\n",
    "        }_{q_\\lambda}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing\n",
    "$$\n",
    "    \\nabla^2_{\\lambda\\omega}\n",
    "        h(\\omega, \\lambda) \\delta\n",
    "        = \\tfrac{\\partial}{\\partial \\lambda}\n",
    "            \\bigl\\{ \\nabla_\\omega^\\top h(\\omega, \\lambda) \\delta \\bigr \\}\n",
    "        = \\tfrac{\\partial}{\\partial \\lambda} \\ell_\\delta(\\omega)\n",
    "        = d\\bigl\\{ dh(\\omega, \\lambda)(\\delta) \\bigr \\}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_lambda(q):\n",
    "    # get \\nabla_\\omega L_train(\\omega^*(\\lambda), \\lambda)\n",
    "    loss_h = loss(model, *train.tensors)\n",
    "    grad_h_omega = torch.autograd.grad(loss_h, model.parameters(), create_graph=True)\n",
    "\n",
    "    # \\lambda \\mapsto \\nabla_\\omega^\\top h(\\omega, \\lambda) q\n",
    "    grad_h_vect = ddot(grad_h_omega, q)\n",
    "\n",
    "    # get \\nabla^2_{\\lambda\\omega} L_train(\\omega^*(\\lambda), \\lambda) q\n",
    "    hesv_h_lambda = torch.autograd.grad(grad_h_vect, alpha, retain_graph=False)\n",
    "\n",
    "    # get \\nabla_\\lambda L_test(\\omega^*(\\lambda), \\lambda)\n",
    "    loss_g = loss(model, *test.tensors)\n",
    "    grad_g_lambda = torch.autograd.grad(loss_g, alpha, create_graph=False)\n",
    "    \n",
    "    # return daxpy(-1, hesv_h_lambda, grad_g_lambda)\n",
    "    return grad_g_lambda, hesv_h_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad, hesv = grad_lambda(x)\n",
    "grad, hesv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-3\n",
    "alpha.data -= eta * hesv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}(\\omega, \\lambda)\n",
    "    = \\tfrac12 \\| y - X \\omega \\|^2_2\n",
    "    + \\tfrac\\lambda2 \\|\\omega\\|^2_2\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\omega \\mathcal{L}\n",
    "    = - X^\\top(y - X\\omega) + \\omega \\lambda\n",
    "    = (X^\\top X + \\lambda I) \\, \\omega - X^\\top y\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\lambda \\mathcal{L}\n",
    "    = \\tfrac12 \\|\\omega \\|^2_2\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\omega \\lambda} \\mathcal{L}\n",
    "    = \\nabla_{\\lambda \\omega} \\mathcal{L}\n",
    "    = \\omega\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $\\omega^*(\\lambda) = (X^\\top X + \\lambda I)^{-1} X^\\top y$, whence\n",
    "\n",
    "$$\n",
    "F(\\lambda)\n",
    "    = \\mathcal{L}(\\omega^*(\\lambda), \\lambda)\n",
    "    = \\tfrac12 \\| y - X \\omega^*(\\lambda) \\|^2_2\n",
    "    + \\tfrac\\lambda2 \\| \\omega^*(\\lambda) \\|^2_2\n",
    "    \\,, $$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nabla_\\lambda F(\\lambda)\n",
    "    = \\tfrac12 \\|\\omega^*(\\lambda)\\|^2_2\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_h = loss(model, *train.tensors)\n",
    "\n",
    "grad_h_omega = torch.autograd.grad(loss_h, model.parameters(), create_graph=True)\n",
    "\n",
    "fn = ddot(grad_h_omega, [p.data for p in model.parameters()])\n",
    "\n",
    "torch.autograd.grad(fn, alpha, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(map(torch.norm, grad_h_omega)), sum(map(torch.norm, model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following setup:\n",
    "$$\n",
    "    l^*(\\lambda)\n",
    "        = \\min_{\\omega} l(\\omega, \\lambda)\n",
    "        = l(\\omega^*(\\lambda), \\lambda)\n",
    "    \\,, $$\n",
    "where\n",
    "$$\n",
    "    \\omega^*(\\lambda) = \\arg \\min_{\\omega} l(\\omega, \\lambda)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose everything is sufficiently smooth, and $l \\neq \\ell$. Then differentiating\n",
    "$f(\\lambda) = \\ell(\\omega^*(\\lambda), \\lambda)$:\n",
    "$$\n",
    "    \\partial_\\lambda f(\\lambda)\n",
    "        = \\partial_1 \\ell(\\omega^*, \\lambda) \\circ \\partial_\\lambda \\omega^*\n",
    "        + \\partial_2 \\ell(\\omega^*, \\lambda)\n",
    "    \\,. $$\n",
    "\n",
    "In finite dimensions we have\n",
    "$$\n",
    "    \\nabla_\\lambda f\n",
    "        = \\nabla_\\lambda \\omega^*(\\lambda) \\nabla_\\omega \\ell(\\omega, \\lambda)\n",
    "            \\big\\vert_{\\omega^*(\\lambda), \\lambda}\n",
    "        + \\nabla_\\lambda \\ell(\\omega, \\lambda)\n",
    "            \\big\\vert_{\\omega^*(\\lambda), \\lambda}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otoh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\nabla_\\lambda l^*(\\lambda)\n",
    "        = \\nabla_\\lambda \\omega^*(\\lambda) \\nabla_\\omega l(\\omega, \\lambda)\n",
    "            \\big\\vert_{\\omega^*(\\lambda), \\lambda}\n",
    "        + \\nabla_\\lambda l(\\omega, \\lambda)\n",
    "            \\big\\vert_{\\omega^*(\\lambda), \\lambda}\n",
    "        = 0\n",
    "    \\,, $$\n",
    "whence\n",
    "$$\n",
    "    \\nabla_\\lambda \\omega^*(\\lambda) \\nabla_\\omega l(\\omega, \\lambda)\n",
    "            \\big\\vert_{\\omega^*(\\lambda), \\lambda}\n",
    "        = - \\nabla_\\lambda l(\\omega, \\lambda)\n",
    "            \\big\\vert_{\\omega^*(\\lambda), \\lambda}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gateaux derivative\n",
    "$$\n",
    "    \\tfrac{d}{d\\eta } \\omega^*(\\lambda + \\eta v) \\big \\vert_{\\eta = 0}\n",
    "        = \\lim_{\\eta \\to 0} \\frac{\n",
    "            \\arg \\min_{\\omega} l(\\omega, \\lambda + \\eta v)\n",
    "            - \\arg \\min_{\\omega} l(\\omega, \\lambda)\n",
    "        }{\\eta}\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import prod\n",
    "\n",
    "class vector(tuple):\n",
    "    def __new__(cls, *tensors):\n",
    "        assert tensors\n",
    "        if isinstance(tensors[0], cls):\n",
    "            return tensors[0]\n",
    "\n",
    "        if isinstance(tensors[0], (tuple, list)):\n",
    "            tensors = tensors[0]\n",
    "\n",
    "        self = super().__new__(cls, tensors)\n",
    "        self.shape = tuple(u.shape for u in self)\n",
    "        return self\n",
    "\n",
    "    def __pos__(self):\n",
    "        return self\n",
    "\n",
    "    def __neg__(self):\n",
    "        return type(self)(*map(torch.neg, self))\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return type(self)(*map(torch.add, self, other))\n",
    "    __radd__ = __add__\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        return type(self)(*map(torch.Tensor.add_, self, other))\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return type(self)(*map(torch.sub, self, other))\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        return type(self)(*map(torch.Tensor.sub_, self, other))\n",
    "\n",
    "    def __mul__(self, alpha):\n",
    "        return type(self)([torch.mul(u, alpha) for u in self])\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def __imul__(self, alpha):\n",
    "        return type(self)([u.mul_(alpha) for u in self])\n",
    "\n",
    "    def __div__(self, other):\n",
    "        return type(self)([torch.div(u, alpha) for u in self])\n",
    "\n",
    "    def __idiv__(self, other):\n",
    "        return type(self)([u.div_(alpha) for u in self])\n",
    "\n",
    "    def clone(self):\n",
    "        return type(self)(*map(torch.clone, self))\n",
    "\n",
    "    def zero_(self):\n",
    "        return type(self)(*map(torch.zero_, self))\n",
    "\n",
    "    def detach(self):\n",
    "        return type(self)(*map(torch.detach, self))\n",
    "\n",
    "    def requires_grad_(self, requires_grad=True):\n",
    "        return type(self)([u.requires_grad_(requires_grad) for u in self])\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        return type(self)([u.to(*args, **kwargs) for u in self])\n",
    "    \n",
    "    def tensor(self):\n",
    "        return torch.cat([u.flatten() for u in self])\n",
    "    \n",
    "    @classmethod\n",
    "    def from_tensor(cls, shapes):\n",
    "        flat = torch.split(a, [*map(prod, shape)])\n",
    "        return cls([u.reshape(s) for u, s in zip(flat, shape)])\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return torch.dot(self.tensor(), other.tensor())\n",
    "\n",
    "    def __abs__(self):\n",
    "        return torch.norm(self.tensor())\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(u.numel() for u in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def trcg_vec(Ax, r, x, n_iterations=1000, tr_delta=0, rtol=1e-5, atol=1e-8,\n",
    "         args=(), verbose=False):\n",
    "    if n_iterations > 0:\n",
    "        n_iterations = min(n_iterations, len(x))\n",
    "\n",
    "    p, iteration = r.clone(), 0\n",
    "    tr_delta_sq = tr_delta ** 2\n",
    "\n",
    "    rtr, rtr_old = float(r @ r), 1.0\n",
    "    cg_tol = sqrt(rtr) * rtol + atol\n",
    "    region_breached = False\n",
    "    while (iteration < n_iterations) and (sqrt(rtr) > cg_tol):\n",
    "        Ap = vector(Ax(p, *args))\n",
    "        iteration += 1\n",
    "        if verbose:\n",
    "            print(\"\"\"iter %2d |Ap| %5.3e |p| %5.3e \"\"\"\n",
    "                  \"\"\"|r| %5.3e |x| %5.3e beta %5.3e\"\"\" %\n",
    "                  (iteration, abs(Ap), abs(p), abs(r), abs(x), rtr / rtr_old))\n",
    "        # end if\n",
    "\n",
    "        alpha = rtr / float(p @ Ap)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "\n",
    "        # check trust region (diverges from tron.cpp in liblinear and leml-imf)\n",
    "        if tr_delta_sq > 0:\n",
    "            xTx = float(x @ x)\n",
    "            if xTx > tr_delta_sq:\n",
    "                xTp = float(x @ p)\n",
    "                if xTp > 0:\n",
    "                    # backtrack into the trust region\n",
    "                    p_nrm = abs(p)\n",
    "\n",
    "                    q = xTp / p_nrm\n",
    "                    eta = (q - sqrt(max(q * q + tr_delta_sq - xTx, 0))) / p_nrm\n",
    "\n",
    "                    # reproject onto the boundary of the region\n",
    "                    r += eta * Ap\n",
    "                    x -= eta * p\n",
    "                else:\n",
    "                    # this never happens maybe due to CG iteration properties\n",
    "                    pass\n",
    "                # end if\n",
    "\n",
    "                region_breached = True\n",
    "                break\n",
    "            # end if\n",
    "        # end if\n",
    "\n",
    "        rtr, rtr_old = float(r @ r), rtr\n",
    "        p *= rtr / rtr_old\n",
    "        p += r\n",
    "    # end while\n",
    "\n",
    "    return iteration, region_breached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, r = vector(grad_g_omega).clone().zero_(), vector(grad_g_omega).clone()\n",
    "print(trcg_vec(Ax, r, x, rtol=1e-9, atol=1e-9, verbose=True))\n",
    "\n",
    "assert all(map(torch.allclose, Ax(x), grad_g_omega))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Term(object):\n",
    "    def __init__(self):\n",
    "        self.losses = set()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        self.losses.add(other)\n",
    "        return self\n",
    "    \n",
    "    def __call__(self, model, X, y):\n",
    "        if self.losses:\n",
    "            return sum(loss.forward(model, X, y) for loss in self.losses)\n",
    "        return self.forward(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Term):\n",
    "    def forward(self, model, X, y):\n",
    "        return 0.5 * F.mse_loss(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting alphas and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(module, prefix=\"\"):\n",
    "    for name, par in module.named_parameters(prefix):\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gumbel-softmax trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    prob_t = tt.nnet.softmax(logit_t)\n",
    "\n",
    "    # Gumbel-softmax sampling: Gumbel (e^{-e^{-x}}) distributed random noise\n",
    "    gumbel = -tt.log(-tt.log(theano_random_state.uniform(size=logit_t.shape) + eps) + eps)\n",
    "#     logit_t = theano.ifelse.ifelse(tt.gt(tau, 0), gumbel + logit_t, logit_t)\n",
    "#     inv_temp = theano.ifelse.ifelse(tt.gt(tau, 0), 1.0 / tau, tt.constant(1.0))\n",
    "    logit_t = tt.switch(tt.gt(tau, 0), gumbel + logit_t, logit_t)\n",
    "    inv_temp = tt.switch(tt.gt(tau, 0), 1.0 / tau, tt.constant(1.0))\n",
    "\n",
    "    # Get the softmax: x_t is `BxV`\n",
    "    x_t = tt.nnet.softmax(logit_t * inv_temp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1e-1\n",
    "logit = torch.randn(10, 10)\n",
    "\n",
    "gumbel = -torch.log(-torch.log(torch.rand_like(logit)))\n",
    "\n",
    "proba = torch.softmax((logit + gumbel) / temp, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
