{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A study of SDE and stoch processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions that need answering:\n",
    "* what is the space setup here? bounded continuous function of $[0, T]$? or is it the dual of that (some good behaving measures)? What is the dual space?\n",
    "* what is the norm/metric/topology here for strong convergence?\n",
    "* is the weak convergence here the same thing as weak-$\\star$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's rewrite the below by summarizing [Higham D.J (2002)](http://homepages.warwick.ac.uk/~masdr/JOURNALPUBS/stuart51.pdf),\n",
    "[this doctoral thesis](https://core.ac.uk/download/pdf/70597247.pdf)\n",
    "and [this paper on Euler-Maryama](https://arxiv.org/abs/1610.07047.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider and SDE\n",
    "$$\n",
    "dX_t\n",
    "    = \\mu_t dt + \\Sigma_t^{\\tfrac12} dW_t\n",
    "    \\,, $$\n",
    "where $\\mu_t = \\mu(X_t)$, $\\Sigma_t = \\Sigma(X_t)$ and $W_t$\n",
    "is a multivaraite Brownian motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euler-Maruyama numerical integration method of this SDE goes like this\n",
    "(based on this lecture [on numerical methods in Computational Finance](https://www.mimuw.edu.pl/~apalczew/CFP_lecture5.pdf)).\n",
    "\n",
    "The SDE is in fact the following integral equation\n",
    "$$\n",
    "X_t - X_0\n",
    "    = \\int_0^t \\mu_\\tau d\\tau + \\int_0^t \\Sigma_\\tau^{\\tfrac12} dW_\\tau\n",
    "  \\,, $$\n",
    "\n",
    "so for $t$ and $t+\\delta$ the ÃŽto integral of the BM can be approximated\n",
    "by the same finite difference, with which it is constructed (in $\\ell_2$)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int_t^{t+\\delta} \\Sigma_\\tau^{\\tfrac12} dW_\\tau\n",
    "    &\\approx \\Sigma_t^{\\tfrac12} \\bigl(W_{t+\\delta} - W_t\\bigr)\n",
    "    \\,, \\\\\n",
    "\\int_t^{t+\\delta} \\mu_\\tau d\\tau\n",
    "    & \\approx \\mu_t \\delta\n",
    "    \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "where approximations are in $\\ell_2$ ($\\|\\cdot\\|_2^2 = \\mathbb{E}(\\cdot)^2$)\n",
    "sense as $\\delta \\to 0$. Therefore the $\\delta$-difference obeys\n",
    "\n",
    "$$\n",
    "X_{t+\\delta} - X_t\n",
    "    \\sim \\mathcal{N}_d(\\mu_t \\delta, \\Sigma_t \\delta)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a partition $0 = t_0 < t_1 < \\cdots < t_n = T$ with $\n",
    "\\max_{k=1}^n (t_{k+1} - t_k) \\leq \\delta\n",
    "$\n",
    "and denote by\n",
    "$ X^\\delta_t $ the piecewise linear interpolation of $(X_{t_n})_{n=0}^n$:\n",
    "$$\n",
    "X^\\delta_t\n",
    "%     = \\sum_{k=1}^n\n",
    "%         1_{[t_{k-1}, t_k)}(t) \\Bigl(\n",
    "%             X_{t_k} + \\frac{t - t_k}{t_{k+1} - t_k} (X_{t_{k+1}} - X_{t_k})\n",
    "%         \\Bigr)\n",
    "    = \\sum_{k \\colon t \\in [t_{k-1}, t_k)}\n",
    "        X_{t_k} + \\frac{t - t_k}{t_{k+1} - t_k} (X_{t_{k+1}} - X_{t_k})\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A numerical scheme for a given SDE has strong convergence of order $\\gamma$\n",
    "if there exists $C_T$ (depending on the SDE and time $T$) such that\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\biggl(\n",
    "    \\sup_{t \\in [0, T]} \\bigl\\| X_t - X^\\delta_t \\bigr\\|_2^2\n",
    "\\biggr)^{\\tfrac12}\n",
    "    \\leq C_T \\delta^\\gamma\n",
    "    \\,. $$\n",
    "\n",
    "Another notion of convergence for SDEs (and in general in probability) is weak\n",
    "(weak-$\\star$ in functional analysis). A scheme for an SDE has weak convergence\n",
    "of order $\\gamma$ if for any bounded continuous function $\n",
    "    g \\in C^\\infty(\n",
    "        \\mathbb{R}^d \\to \\mathbb{R}\n",
    "    )\n",
    "$ there is a $C_{T,g}$ such that\n",
    "\n",
    "$$\n",
    "\\bigl\\lvert\n",
    "    \\mathbb{E} g(X_T) - g(X^\\delta_T)\n",
    "\\bigr\\rvert\n",
    "    \\leq C_{T,g} \\delta^\\gamma\n",
    "    \\,. $$\n",
    "Note that weak convergence concerns the distribution at $T$ only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euler_maruyama(mu, sigma, x_0, *, delta=1e-4):\n",
    "    X_t = x_0.clone().detach()\n",
    "    *head, _ = X_t.shape\n",
    "\n",
    "    mu_t, sigma_t = mu(X_t), sigma(X_t)\n",
    "    while True:\n",
    "        X_t.add_(mu_t * delta)  # make a mul-copy of mu\n",
    "\n",
    "        # torch.matmul(dW_t, torch.cholesky(sigma_t, upper=True))\n",
    "        dW_t = torch.randn(*head, sigma_t.shape[-1], 1)\n",
    "        dW_t.mul_(math.sqrt(delta))\n",
    "\n",
    "        X_t.add_(torch.matmul(sigma_t, dW_t).squeeze(-1))\n",
    "\n",
    "        yield X_t.clone()\n",
    "\n",
    "        mu_t, sigma_t = mu(X_t), sigma(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 20)\n",
    "sigma_t = torch.cholesky(torch.mm(a, a.t()) * 1e-2, upper=False)\n",
    "mu_t = torch.zeros(5, 2) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [torch.randn(1, 2).repeat(5, 1) * 1e-1]\n",
    "# paths = [torch.randn(5, 2) * 1e-1]\n",
    "int_ = euler_maruyama(lambda x: mu_t, lambda x: sigma_t,\n",
    "                      paths[-1], delta=1e-3)\n",
    "\n",
    "paths.extend(X_t for _, X_t in zip(range(200), int_))\n",
    "\n",
    "paths = torch.stack(paths, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 14))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "colors = plt.cm.Accent(np.linspace(0, 1, num=len(paths)))\n",
    "for path, col in zip(paths, colors):\n",
    "    path = path.numpy()\n",
    "    xy, uv = path[:-1], path[1:] - path[:-1]\n",
    "    ax.quiver(xy[:, 0], xy[:, 1], uv[:, 0], uv[:, 1], color=col,\n",
    "              angles=\"xy\", units=\"xy\", scale=1., scale_units=\"xy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(torch.cholesky(a, upper=True).t(), torch.cholesky(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we could start with the simple ode of the form\n",
    "$$ \\label{eq:prob_3_2}\n",
    "\\frac{dy}{dx} = f(x, y)\n",
    "    \\,, y(x_0) = y_0\n",
    "    \\,. \\tag{3.2}\n",
    "    $$\n",
    "\n",
    "[p. 42 in Petrovskiy (1984)](#.pdf) :\n",
    "\n",
    "If $f\\colon G \\to \\mathbb{R}$ is bounded and continuous on a domain $\n",
    "  G \\subset \\mathbb{R}\\times \\mathbb{R}\n",
    "$ (open connected subset), then for any $(x_0, y_0) \\in G$ there exists\n",
    "at least one solution (integral curve) $\n",
    "  \\phi \\colon [a, b] \\to \\mathbb{R}\n",
    "$ with $a < x_0 < b$ and $\n",
    "  [a, b] \\subset \\{x\\colon (x, y) \\in G\\}\n",
    "$ satisying the problem \\eqref{eq:prob_3_2}. If $f$ is (uniformly) Lipschitz\n",
    "in $y$ then the solutons are unique.\n",
    "\n",
    "[p. 57 in Petrovskiy (1984)](#.pdf):\n",
    "\n",
    "If the map $f(x, y) \\colon G \\to \\mathbb{R}^d$ is continuous on a domain $\n",
    "  G \\subset \\mathbb{R} \\times \\mathbb{R}^d\n",
    "$ w.r.t. $x$ and Lischitz on any closed bounded (compact?) subset of $G$,\n",
    "then for any $(x_0, y_0) \\in G$ there exists a closed interval $[a, b]$\n",
    "inside $G$ covering $x_0$ over which a unique solution to the problem\n",
    "\\eqref{eq:prob_3_2} is defined. (Contraction mapping theorem)\n",
    "\n",
    "[p. 78 Petrovskiy (1984)](#.pdf) :\n",
    "\n",
    "If in \\eqref{eq:prob_3_2} $f$ is $p$-smooth w.r.t both $x$ and $y$ then the solutions are $p+1$ smooth.\n",
    "\n",
    "[p. 80 Petrovskiy (1984)](#.pdf) :\n",
    "\n",
    "If $f$ continuous, bounded, and every solution to \\eqref{eq:prob_3_2} is unique,\n",
    "then solutions are continuous w.r.t. $x_0, y_0$ and $f$ ($\\sup$-norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=1e-2)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "f_preds = model(test_x)\n",
    "y_preds = likelihood(model(test_x))\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_var = f_preds.variance\n",
    "f_covar = f_preds.covariance_matrix\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size(1000,))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    mv = model(test_x)\n",
    "\n",
    "smpl = mv.sample(torch.Size((11,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "ax.plot(test_x, smpl.numpy().T);\n",
    "ax.plot(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
